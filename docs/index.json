[{"content":"仕事(エンジニア/フルリモート)に役立ったかどうか、という観点で備忘的にまとめてみました。\n自宅サーバー (自作) サーバーとして使っていたRaspberry piが成仏し、現在どこも在庫が皆無なため代わりにファンレスPCを組みました。スペック的にも物理的な拡張性にも余裕があるためより一層遊べるようになり、結果として大正解。\nサブスクのクラウドサービスをセルフホステッドにどんどん切り替えていってるので、スキルアップと家計の両面に大いに貢献しています。\n消費電力は25W/hくらい、電気代は月500円弱。 動かしているアプリケーションに関してはまた別記事で書きたい。\n   パーツ 型番     M/B BIOSTAR J4105NH   CPU オンボード   MEM CFD W4U2400PS-4GC17 (4GB x 2)   PSU DC-DCコンバータ + 120W ACアダプタ   ストレージ 余っているSSD/HDDいっぱい。合計10TBくらい   ケース 100均の書類ケースに適当にネジ止め    自作キーボード YD60MQ 何個かキーボードを作ってみて得た結論として、\n 特殊すぎる配列はメリットに対して切り替えコストが割に合わない 右手でマウスを持ったまま左手で右側のキーを押す事があるため、自分の場合は左右分割だと効率が下がる  という事から、結局HHKB配列の YMDK YD60MQ に落ち着きました。\nCherry MX銀軸が自分にかなりマッチしており、HHKBを使っていたときより指の疲労が低減しました。\nLG ウルトラワイドモニター 35WN75C-B 買ったというか、転職サイトから貰った。\n4kモニタは既に持っていてこちらの方が解像度は低かった(3440×1440)けど、大きい正方形のウィンドウ2つを横に並べても視野に収まるのが予想以上に便利でメインモニタに昇格しました。\n曲面のため光の映り込みが少ないのも便利。\nつっぱり棚 サバゲーやる人が銃を飾るのによく使っているアレです。\n棚が小物類で散らかりまくってカオスだったので、いっそ全てを壁に引っ掛けるスタイルにしました。\nヘッドホンとかメガネとか、100均のS字フックでなんでも引っ掛けています。\n適当に積む事が物理的にできなくなりました。負債を貯めないように頑張るのではなく、負債が貯まらない仕組みができました。\nMi Band 6 安くて小型軽量、更にめちゃくちゃ電池が持つスマートウォッチです。 Apple Watchをあまり着けなくなった過去がありますが、こいつは1週間以上電池が持つので基本つけっぱなしです。\n運動量と睡眠の可視化、Slackの通知などに使っています。\nパネルヒーター ファンヒーターより気持ちいい暖かさで消費電力も少ない。\n暖房を普段より弱くして頭寒足熱状態にすると集中しやすい。\n底面にもパネルがあるタイプがお勧め。\nコーヒーメーカー 誰でもわかるレベルでインスタントコーヒーより美味しく、エナジードリンク依存を脱却できました。\n手動ミル + ペーパーフィルター + コーヒーメーカー(ドリップのみ) のスタックで運用しています。\n 電動ミル → うるさすぎ メッシュフィルター → 豆カスの掃除が面倒、ペーパーフィルターなら捨てるだけ  という事で全自動コーヒーメーカーにはしませんでしたが、今後変わるかも。\n 今年は健康管理関連により一層投資していきたい所存。\n","permalink":"https://aaaanwz.github.io/post/2022/bestbuy2021/","summary":"仕事(エンジニア/フルリモート)に役立ったかどうか、という観点で備忘的にまとめてみました。\n自宅サーバー (自作) サーバーとして使っていたRaspberry piが成仏し、現在どこも在庫が皆無なため代わりにファンレスPCを組みました。スペック的にも物理的な拡張性にも余裕があるためより一層遊べるようになり、結果として大正解。\nサブスクのクラウドサービスをセルフホステッドにどんどん切り替えていってるので、スキルアップと家計の両面に大いに貢献しています。\n消費電力は25W/hくらい、電気代は月500円弱。 動かしているアプリケーションに関してはまた別記事で書きたい。\n   パーツ 型番     M/B BIOSTAR J4105NH   CPU オンボード   MEM CFD W4U2400PS-4GC17 (4GB x 2)   PSU DC-DCコンバータ + 120W ACアダプタ   ストレージ 余っているSSD/HDDいっぱい。合計10TBくらい   ケース 100均の書類ケースに適当にネジ止め    自作キーボード YD60MQ 何個かキーボードを作ってみて得た結論として、\n 特殊すぎる配列はメリットに対して切り替えコストが割に合わない 右手でマウスを持ったまま左手で右側のキーを押す事があるため、自分の場合は左右分割だと効率が下がる  という事から、結局HHKB配列の YMDK YD60MQ に落ち着きました。\nCherry MX銀軸が自分にかなりマッチしており、HHKBを使っていたときより指の疲労が低減しました。\nLG ウルトラワイドモニター 35WN75C-B 買ったというか、転職サイトから貰った。\n4kモニタは既に持っていてこちらの方が解像度は低かった(3440×1440)けど、大きい正方形のウィンドウ2つを横に並べても視野に収まるのが予想以上に便利でメインモニタに昇格しました。\n曲面のため光の映り込みが少ないのも便利。\nつっぱり棚 サバゲーやる人が銃を飾るのによく使っているアレです。","title":"2021年 エンジニア的に買ってよかったもの"},{"content":"2021年6月1日からGoogle Photoの容量が無制限ではなくなり、無料枠は15GBに制限されてしまいました。\n完全に音楽サーバーを構築した話の二番煎じですが、自宅k8sに写真サーバーを構築してそちらに移行する事にしました。\nデプロイ self-hostedな写真サーバーで最もメジャーなプロダクトはPiwigoの模様。\nサクっとyamlを書いてデプロイします。\ndeployment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56  apiVersion: apps/v1 kind: Deployment metadata: name: piwigo spec: replicas: 1 selector: matchLabels: app: piwigo template: metadata: labels: app: piwigo spec: containers: - name: piwigo image: lscr.io/linuxserver/piwigo env: - name: TZ value: Asia/Tokyo - name: PUID value: \u0026#34;1000\u0026#34; - name: PGID value: \u0026#34;1000\u0026#34; ports: - containerPort: 80 protocol: TCP livenessProbe: httpGet: path: / port: 80 volumeMounts: - name: myvolume subPath: piwigo/config mountPath: /config - name: myvolume subPath: piwigo/gallery mountPath: /gallery volumes: - name: myvolume persistentVolumeClaim: claimName: myvolume --- apiVersion: v1 kind: Service metadata: name: piwigo namespace: default spec: ports: - name: http port: 80 protocol: TCP selector: app: piwigo clusterIP: None    別途MySQLも必要なので、Kubernetes公式ドキュメントのサンプルコードあたりを参考にしてデプロイしておきます。\n写真の追加 画像データをmyvolumeの piwigo/gallery/galleries/{アルバム名}/{サブアルバム名}... 以下に追加し、Web UIのAdminメニューから Synchronizeを実行すればアルバムが自動で作成されます。\nこの際、Simulationというdry-run的な機能が用意されており便利です。ファイル名にはスペースや全角文字が使えない等の制約がありますが、Presync AutoRenameというプラグインを使うとある程度は自動でリネームできて便利です。\nもしくは以下のワンライナーでもOK。\n1  $ find . -name \u0026#34;*{置換したい文字列}*\u0026#34; | rename \u0026#39;s/{置換したい文字列}/{置換先文字列}/g\u0026#39;   プラグイン 画像を表示する際にデフォルトでは解像度がかなり落とされるので、Automatic Sizeプラグインを導入しました。\n 他に良さげなプラグインがあれば随時追記予定\n  モバイルアプリもあり、Google Photoからの移行はスムーズに完了しました。\nスマホからの自動同期が無い点だけが不満ですが、容量を気にせず突っ込めるメリットは大きいです。\nクラウドサービスからオンプレへの回帰が進み、自宅サーバーがどんどんミッションクリティカルになってきた\u0026hellip;\n","permalink":"https://aaaanwz.github.io/post/2022/piwigo/","summary":"2021年6月1日からGoogle Photoの容量が無制限ではなくなり、無料枠は15GBに制限されてしまいました。\n完全に音楽サーバーを構築した話の二番煎じですが、自宅k8sに写真サーバーを構築してそちらに移行する事にしました。\nデプロイ self-hostedな写真サーバーで最もメジャーなプロダクトはPiwigoの模様。\nサクっとyamlを書いてデプロイします。\ndeployment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56  apiVersion: apps/v1 kind: Deployment metadata: name: piwigo spec: replicas: 1 selector: matchLabels: app: piwigo template: metadata: labels: app: piwigo spec: containers: - name: piwigo image: lscr.","title":"Kubernetesに写真サーバーを構築する"},{"content":"神(Google)は「Play Music」と言われた。するとGoogle Play Musicがあった。\n神はそのUXを見て、良しとされた。\n神はまた言われた。「YouTube Musicに移行してください」\nUIは使いづらく、バックグラウンド再生できず、ロードは遅くなり、楽曲メタデータは編集できなくなった。\n神はお休みになった。\n概要 所有している音楽データをアップロードし、インターネット経由で聴くというサービスでしっくりくるものがないため、自宅Kubernetesクラスタに自前で構築してみます。\n 家庭内LANからファイルサーバーとして使える ファイルサーバーにアップロードした音楽データをインターネット経由で聴ける ファイルサイズが大きい楽曲はサーバーサイドでリアルタイムに圧縮して配信する  という要件から、以下のような構成にしてみます。\n 音楽配信サーバーには Airsonicを使います  Ingress(L7ロードバランサー)経由でインターネットに接続します IngressをTLS終端にします   ファイルサーバーとしてSambaを構築します  Airsonicとストレージを共有します LoadBalancer Service(L4ロードバランサー)経由で家庭内LANに接続し、インターネットからは遮断します    構築 1. Storage まず初めに、Podからホストマシンのストレージを使うためのPersistentVolume(PV)とPersistentVolumeClaim(PVC)を作成します。 今回は node1 の /mnt/hdd に音楽データとメタデータ(設定、アカウント情報など)を永続化するとします。\npv.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62  apiVersion: v1 kind: PersistentVolume metadata: name: music spec: capacity: storage: 1000Gi accessModes: - ReadWriteOnce local: path: /mnt/hdd/music nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - node1 --- apiVersion: v1 kind: PersistentVolume metadata: name: config spec: capacity: storage: 1Gi accessModes: - ReadWriteOnce local: path: /mnt/hdd/config nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.io/hostname operator: In values: - node1 --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: music spec: accessModes: - ReadWriteOnce resources: requests: storage: 1000Gi --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: config spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi    サンプルコードでは local volume にしているため、この後にデプロイするAirsonicとsambaはnodeAffinityによって node1 にスケジューリングされる事になります。 物理ストレージが接続されているノードとは別のノードでPodを稼働させたい場合はnfs volumeにすればOKです。 (OS側のNFSマウント設定については割愛します)\n1 2 3 4 5 6 7 8 9  ... kind: PersistentVolume ... accessModes: - ReadWriteMany nfs: path: /music server: 192.168.0.XXX ...   2. Samba 次のステップではsambaをデプロイし、k8sクラスタをファイルサーバーとして使えるようにします。\nDeployment sambaで一番人気のDocker imageであるdperson/samba をデプロイします。 上で作成したPVCを /music にマウントします。\nsamba-deployment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42  apiVersion: apps/v1 kind: Deployment metadata: name: samba namespace: default spec: replicas: 1 selector: matchLabels: app: samba template: metadata: labels: app: samba spec: containers: - name: samba image: dperson/samba args: [ \u0026#34;-u\u0026#34;, \u0026#34;myuser;mypassword\u0026#34;, # UPDATE HERE \u0026#34;-s\u0026#34;, \u0026#34;music;/music;yes;no;no;myuser\u0026#34;, \u0026#34;-w\u0026#34;, \u0026#34;WORKGROUP\u0026#34; ] ports: - containerPort: 139 protocol: TCP - containerPort: 445 protocol: TCP resources: limits: cpu: \u0026#34;250m\u0026#34; memory: \u0026#34;500Mi\u0026#34; livenessProbe: tcpSocket: port: 445 volumeMounts: - name: music mountPath: /music volumes: - name: music persistentVolumeClaim: claimName: music    MetalLB PodをLAN内に公開するため、L4ロードバランサーに相当するLoadBalancer Serviceもデプロイします。 事前準備としてオンプレミス環境用のLoadBalancer実装であるMetalLBを導入します。\n　microk8sの場合は以下のコマンド一発で利用可能になります。\n1  $ sudo microk8s.enable metallb:192.168.0.AAA-192.168.0.BBB    : の後はロードバランサーが用いるローカルIPアドレスプールを入力します。\n その他の環境では公式ドキュメントを参照してください。\nService samba Podの139番と445番ポートをローカルIPアドレス192.168.0.YYYとしてクラスタ外に公開します。 (YYYはMetalLBのアドレスプールの範囲で未使用のアドレスを設定します)\nsamba-svc.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  apiVersion: v1 kind: Service metadata: name: samba namespace: default spec: selector: app: samba type: LoadBalancer loadBalancerIP: 192.168.0.YYY # UPDATE HERE ports: - name: netbios port: 139 targetPort: 139 - name: smb port: 445 targetPort: 445    EXTERNAL-IPが割り当てられている事を確認します。\n1 2 3  $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE samba LoadBalancer 10.aaa.bbb.ccc 192.168.0.YYY 139:3xxxx/TCP,445:3xxxx/TCP 1m    Windows PCであればエクスプローラの ネットワークドライブの割り当て から、¥¥192.168.0.YYY¥music をネットワークドライブとしてマウントできるようになります。 手持ちの音楽データを {アーティスト名}/{アルバム名}/{曲名}.{ファイル形式} のディレクトリ構成でアップロードしておきます。\n3. Airsonic 本題である音楽ストリーミングサーバーのAirsonicをデプロイします。\nDeployment linuxserver/airsonic のDockerhubドキュメントに沿って実装していきます。 (公式のairsonic/airsonic イメージはメンテナンスされていないようでした)\nsanbaでも用いた music PVCを /music にマウントします。 またメタデータはコンテナ内の /config に保存されるため、config PVCを /config にマウントしてこれらのデータが永続化されるようにしておきます。　 Airsonicは内蔵DB(HSQLDB)だけでなくPostgresやMySQLなどの外部DBにも対応しています。 外部DBをStatefulSetとしてk8s上に構築すればAirsonicをstatelessにできて良さそうですが、パフォーマンスに問題を抱えているため今回は見送りました。\n airsonic-deployment.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49  apiVersion: apps/v1 kind: Deployment metadata: name: airsonic namespace: default spec: replicas: 1 selector: matchLabels: app: airsonic template: metadata: labels: app: airsonic spec: containers: - name: airsonic image: lscr.io/linuxserver/airsonic env: - name: TZ value: Asia/Tokyo - name: PUID value: \u0026#34;1000\u0026#34; - name: PGID value: \u0026#34;1000\u0026#34; ports: - containerPort: 4040 protocol: TCP resources: limits: cpu: \u0026#34;2\u0026#34; memory: \u0026#34;2Gi\u0026#34; livenessProbe: httpGet: path: /login port: 4040 initialDelaySeconds: 60 # CrashLoopBackOffになる場合はこの値を大きくする volumeMounts: - name: config mountPath: /config - name: music mountPath: /music volumes: - name: music persistentVolumeClaim: claimName: music - name: config persistentVolumeClaim: claimName: config    Service Sambaとは異なりServiceをクラスタ外に公開する必要は無いため、Headless Serviceとして実装します。\nairsonic-svc.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14  apiVersion: v1 kind: Service metadata: name: airsonic namespace: default spec: selector: app: airsonic type: ClusterIP clusterIP: None ports: - name: http port: 4040 protocol: TCP    Nginx Ingress Controller AirsonicはWebアプリケーションのため、L7ロードバランサーに相当するIngressを経由してServiceをクラスタ外に公開します。 事前準備としてオンプレミス環境用のIngress実装であるNginx Ingress Controllerを導入します。 導入手順は公式ドキュメントを参照してください。 (microk8sであれば sudo microk8s.enable ingress で終わりです)\nIngress クラスターエンドポイントIPへのhttpアクセスをairsonicのServiceに転送する設定でIngressをデプロイします。 https化は次のステップで実施します。\nairsonic-ingress-http.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: airsonic-http namespace: default spec: rules: - http: paths: - path: / pathType: Prefix backend: service: name: airsonic port: number: 4040    1 2 3  $ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE airsonic-http public * 127.0.0.1 80 1m    LAN内PCのブラウザから http://{クラスターエンドポイントIP} を開くとログイン画面が表示されます。 初期ID/パスワードはadmin/adminなのでログイン後すぐに変更しましょう。 ログインするとsamba経由でアップロードした音楽を聴くことができるようになっています。\n4. https化 LAN内からAirsonicを利用できるようになりました。 次は最後のステップとしてAirsonic Podへの接続をhttps化し、インターネットに公開します。\nDNS設定 ルータのグローバルIPと自身の所有するドメイン(以降 example.comと表記)を紐づけるAレコードをDNSに追加します。 筆者は無料のダイナミックDNSサービスを使っています。\nルータの設定 クラスターエンドポイントIP(192.168.0.XXX)の80番と443番ポートを開放します。\n この時点で http://example.com としてAirsonicにアクセスできるようになります。\n cert-manager TLS証明書や発行者をk8sリソースとして管理可能にするcert-managerを導入します。\n1  $ kubectl apply -f https://github.com/jetstack/cert-manager/releases/download/v1.6.1/cert-manager.yaml   Issuer 証明書発行者(Let\u0026rsquo;s Encrypt)をk8sクラスタに追加します。 Let\u0026rsquo;s Encryptの本番環境で試行錯誤しているとレート制限に引っかかる可能性があるため、検証用にステージング環境も追加します。 spec.acme.solvers.http01.ingress.class の値は環境に応じて設定してください。\nissuer.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: letsencrypt-staging spec: acme: server: https://acme-staging-v02.api.letsencrypt.org/directory email: myaddress@example.com # UPDATE HERE privateKeySecretRef: name: letsencrypt-staging solvers: - http01: ingress: class: public # UPDATE HERE --- apiVersion: cert-manager.io/v1 kind: Issuer metadata: name: letsencrypt-prod spec: acme: server: https://acme-v02.api.letsencrypt.org/directory email: myaddress@example.com # UPDATE HERE privateKeySecretRef: name: letsencrypt-prod solvers: - http01: ingress: class: public # UPDATE HERE    Ingressを更新 Ingressをhttps対応に書き換えます。先ほど作成したIngressを削除し、以下の設定で再作成します。\n1 2  $ kubectl delete ingress airsonic-http $ kubectl create -f airsonic-ingress.yaml   airsonic-ingress.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: airsonic namespace: default annotations: cert-manager.io/issuer: \u0026#34;letsencrypt-staging\u0026#34; spec: tls: - hosts: - example.com secretName: tls rules: - host: example.com http: paths: - path: / pathType: Prefix backend: service: name: airsonic port: number: 4040    Ingressを作成すると証明書リクエストが実施されます。リクエストは CertificateRequest としてk8sリソースになっています。\n1 2 3  $ kubectl get cr NAME APPROVED DENIED READY ISSUER REQUESTOR AGE tls-xxxxx True True letsencrypt-staging system:serviceaccount:cert-manager:cert-manager 1m   APPROVED: True になると証明書がREADYになります。\n1 2 3  $ kubectl get cert NAME READY SECRET AGE tls True tls 1m    しばらく待っても証明書が取得できない場合はkubectl describe cr や kubectl describe order で原因を調査します。 example.com:80 へのチャレンジによって認証されるため、ルーターの設定などが原因になりがちです。\n Let\u0026rsquo;s Encryptステージング環境で正しく動作する事を確認したら、issuerを本番に切り替えて再作成します。\n1 2 3 4  ... - cert-manager.io/issuer: \u0026#34;letsencrypt-staging\u0026#34; + cert-manager.io/issuer: \u0026#34;letsencrypt-prod\u0026#34; ...   1 2 3  $ kubectl get ingress NAME CLASS HOSTS ADDRESS PORTS AGE airsonic public example.com 127.0.0.1 80, 443 1m    証明書の期限が近づくと自動で更新されます。\n Airsonic設定ファイルの編集 ChromeからAirsonic Web UIにhttpsでアクセスすると、一部要素がMixed Contentsでブロックされてしまいます。 AirsonicのPodに入り、X-Forwarded-For を有効化する設定を追記してPodを再起動します。\n1 2 3 4  $ kubectl exec -it airsonic-xxxxxxx bash # echo \u0026#34;server.use-forward-headers=true\u0026#34; \u0026gt;\u0026gt; /config/airsonic.properties # exit $ kubectl rollout restart deployments/airsonic   以上でインターネットから https://example.com でAirsonicに接続できるようになります。\nおわりに AirsonicはSubsonicというプロダクトからフォークされたもので、Subsonic APIに対応する様々なクライアントを利用することができます。 筆者はAndroidでは Ultrasonic、iOSでは iSub というアプリを利用しています。\n サンプルコードは最小限の実装になっていますので、実運用では環境に応じて冗長化やセキュリティ面の設定などを追加してください。\n","permalink":"https://aaaanwz.github.io/post/2021/airsonic/","summary":"神(Google)は「Play Music」と言われた。するとGoogle Play Musicがあった。\n神はそのUXを見て、良しとされた。\n神はまた言われた。「YouTube Musicに移行してください」\nUIは使いづらく、バックグラウンド再生できず、ロードは遅くなり、楽曲メタデータは編集できなくなった。\n神はお休みになった。\n概要 所有している音楽データをアップロードし、インターネット経由で聴くというサービスでしっくりくるものがないため、自宅Kubernetesクラスタに自前で構築してみます。\n 家庭内LANからファイルサーバーとして使える ファイルサーバーにアップロードした音楽データをインターネット経由で聴ける ファイルサイズが大きい楽曲はサーバーサイドでリアルタイムに圧縮して配信する  という要件から、以下のような構成にしてみます。\n 音楽配信サーバーには Airsonicを使います  Ingress(L7ロードバランサー)経由でインターネットに接続します IngressをTLS終端にします   ファイルサーバーとしてSambaを構築します  Airsonicとストレージを共有します LoadBalancer Service(L4ロードバランサー)経由で家庭内LANに接続し、インターネットからは遮断します    構築 1. Storage まず初めに、Podからホストマシンのストレージを使うためのPersistentVolume(PV)とPersistentVolumeClaim(PVC)を作成します。 今回は node1 の /mnt/hdd に音楽データとメタデータ(設定、アカウント情報など)を永続化するとします。\npv.yaml 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62  apiVersion: v1 kind: PersistentVolume metadata: name: music spec: capacity: storage: 1000Gi accessModes: - ReadWriteOnce local: path: /mnt/hdd/music nodeAffinity: required: nodeSelectorTerms: - matchExpressions: - key: kubernetes.","title":"おうちKubernetesに音楽ストリーミングサーバー(兼ファイルサーバー)を構築する"},{"content":"M1 Macに移行する気になれなかったのでメインマシンをx86 + Ubuntu Desktop 20.04にしました。\nGPUドライバのインストール 1  $ sudo ubuntu-drivers autoinstall   IME切り替えショートカットを Ctrl + Space に設定  設定 \u0026gt; 地域と言語 \u0026gt; 入力ソース で入力ソースを 日本語(Mozc) のみにする Mozcプロパティ \u0026gt; キー設定 \u0026gt; 編集　で 入力キーがCtrl Spaceのエントリーを削除し、以下のように設定  CapsLockをCtrlに変更 1 2 3  $ sudo vi /etc/default/keyboard ... XKBOPTIONS=\u0026#34;ctrl:nocaps\u0026#34;   Chromeをダークモード化 1 2 3  $ sudo vi /opt/google/chrome/google-chrome ... exec -a \u0026#34;$0\u0026#34; \u0026#34;$HERE/chrome\u0026#34; \u0026#34;--enable-features=WebUIDarkMode\u0026#34; \u0026#34;--force-dark-mode\u0026#34; \u0026#34;$@\u0026#34;   日本語ディレクトリを変更 1 2 3 4 5 6 7 8 9 10  $ sudo vi .config/user-dirs.dirs XDG_DESKTOP_DIR=\u0026#34;$HOME/Desktop\u0026#34; XDG_DOWNLOAD_DIR=\u0026#34;$HOME/Downloads\u0026#34; XDG_TEMPLATES_DIR=\u0026#34;$HOME/Templates\u0026#34; XDG_PUBLICSHARE_DIR=\u0026#34;$HOME/Share\u0026#34; XDG_DOCUMENTS_DIR=\u0026#34;$HOME/Documents\u0026#34; XDG_MUSIC_DIR=\u0026#34;$HOME/Music\u0026#34; XDG_PICTURES_DIR=\u0026#34;$HOME/Pictures\u0026#34; XDG_VIDEOS_DIR=\u0026#34;$HOME/Videos\u0026#34;   デスクトップとファイルマネージャの間でファイルの移動ができるようにする GNOME拡張のDesktop Icons NG (DING)をインストール https://extensions.gnome.org/extension/2087/desktop-icons-ng-ding/\n元々のDesktop Iconsを無効化 デスクトップアイコンが二重に表示されるので、元々の機能を無効化する\n1  $ sudo apt install gnome-shell-extension-prefs   メニュー \u0026gt; 拡張機能 でDesktop Iconsをオフにする\nDocker sudo無しで使えるようにgroupにユーザーを入れる\nhttps://docs.docker.com/engine/install/linux-postinstall/\n1 2 3 4  $ sudo snap install docker $ sudo groupadd docker $ sudo usermod -aG docker $USER $ newgrp docker   Wine https://wiki.winehq.org/Ubuntu\n1 2 3 4 5 6 7  $ sudo dpkg --add-architecture i386 $ wget -nc https://dl.winehq.org/wine-builds/winehq.key $ sudo apt-key add winehq.key $ sudo add-apt-repository \u0026#39;deb https://dl.winehq.org/wine-builds/ubuntu/ focal main\u0026#39; $ sudo apt update $ sudo apt install wine-development $ winecfg   文字化け対策\n1 2 3 4  $ sudo apt-get install winetricks $ winetricks fakejapanese $ winetricks corefonts $ winetricks cjkfonts   3D関係 1  $ sudo apt install vulkan-utils dxvk   以下のファイルを作成する\n1 2 3 4 5 6 7 8 9  $ vi ~/.wine/windows/winevulkan.json { \u0026#34;file_format_version\u0026#34;: \u0026#34;1.0.0\u0026#34;, \u0026#34;ICD\u0026#34;: { \u0026#34;library_path\u0026#34;: \u0026#34;c:\\\\windows\\\\system32\\\\winevulkan.dll\u0026#34;, \u0026#34;api_version\u0026#34;: \u0026#34;1.0.51\u0026#34; } }   1  $ dxvk-setup i   VSCode snapで入れるとKubernetes extensionが動作しないため、公式の.debパッケージを入れる\nSlack snapで入れると日本語入力ができないため、公式の.devパッケージを入れる https://slack.com/intl/ja-jp/downloads/linux\n GUIアプリはsnap版に不具合があるケースが多い模様。\n随時追記予定\n","permalink":"https://aaaanwz.github.io/post/2021/ubuntu-desktop-config/","summary":"M1 Macに移行する気になれなかったのでメインマシンをx86 + Ubuntu Desktop 20.04にしました。\nGPUドライバのインストール 1  $ sudo ubuntu-drivers autoinstall   IME切り替えショートカットを Ctrl + Space に設定  設定 \u0026gt; 地域と言語 \u0026gt; 入力ソース で入力ソースを 日本語(Mozc) のみにする Mozcプロパティ \u0026gt; キー設定 \u0026gt; 編集　で 入力キーがCtrl Spaceのエントリーを削除し、以下のように設定  CapsLockをCtrlに変更 1 2 3  $ sudo vi /etc/default/keyboard ... XKBOPTIONS=\u0026#34;ctrl:nocaps\u0026#34;   Chromeをダークモード化 1 2 3  $ sudo vi /opt/google/chrome/google-chrome ... exec -a \u0026#34;$0\u0026#34; \u0026#34;$HERE/chrome\u0026#34; \u0026#34;--enable-features=WebUIDarkMode\u0026#34; \u0026#34;--force-dark-mode\u0026#34; \u0026#34;$@\u0026#34;   日本語ディレクトリを変更 1 2 3 4 5 6 7 8 9 10  $ sudo vi .","title":"Ubuntu desktop初期設定メモ"},{"content":"KubernetesでGitOps運用となると必ず話題になるのがSecretの管理です。\n Sealed Secretsやkubesecなどの手元で暗号化する系 Kubernetes Secrets Store CSI Driverやkubernetes-external-secretsなどの外部シークレットストアから引っ張ってくる系 機密情報だけ別Repoにする  など様々な方法がありますが、学習コストや実運用をイメージするとどのソリューションもしっくり来ませんでした。\nそんな中でIBM社が開発しているArgoCD Vault Pluginを触ってみたところ、ArgoCDのデプロイ時にplaceholderをreplaceするという合理的かつシンプルな仕組みで非常に好感触でした。 (上記でいう「外部シークレットストアから引っ張ってくる系」の一種に該当します) ArgoCD Vault Plugin (以下AVP) は日本語の情報が皆無に等しかったため、布教の目的も込めて導入・運用方法を記載します。\nテスト AVPはbrewからも導入でき、手元で簡単にテストができます。 シークレットストアはAWS Secrets Mangerを使う前提で解説します。\nローカル環境にインストール (Mac) 1  $ brew install argocd-vault-plugin   AWS Secrets Mangerに機密文字列を登録する 1 2  key: my_secret value: foobar   Kubernetes Manifestを作成する Secretの実装は非常に簡単で、\n アノテーションに参照するSecret Managerのパスを記述する Secret Managerのキー名を\u0026lt;\u0026gt; で囲う  だけでOKです。\n1 2 3 4 5 6 7 8  apiVersion: v1 kind: Secret metadata: name: credentials annotations: avp.kubernetes.io/path: \u0026#34;avp/test\u0026#34; data: MY_SECRET: \u0026lt;my_secret | base64encode\u0026gt;   Decryptのテスト 1 2 3 4 5 6 7 8 9 10 11 12 13  $ export AWS_ACCESS_KEY_ID=xxxx $ export AWS_SECRET_ACCESS_KEY=xxxx $ export AWS_REGION=ap-northeast-1 $ export AVP_TYPE=awssecretsmanager $ argocd-vault-plugin generate path/to/secrets.yaml apiVersion: v1 kind: Secret metadata: name: credentials annotations: avp.kubernetes.io/path: avp/test data: MY_SECRET: Zm9vYmFy # == $(echo -n foobar | base64)   placeholderがSecrets Mangerに登録していた値に変換されました！ これで安心してGitHub repoにpushできます。\nArgoCDへのPluginの導入 導入方法は公式ドキュメントの方が参考になると思うので、ここではhelmでArgoCDを導入する場合のサンプルコードを掲載します。\n1. IAM Userの設定 AVPがSecret ManagerにアクセスするためのIAM User credentialが必要です。 このcredentialはAVPで管理できないため、どのようにAVPに渡すかは少し考える必要があります。\n方法1. このcredentialだけは手動でapplyする 1 2 3 4 5 6 7 8 9 10  apiVersion: v1 kind: Secret type: Opaque metadata: name: argocd-vault-plugin-iam namespace: argo data: AWS_ACCESS_KEY_ID: xxxxx AWS_SECRET_ACCESS_KEY: xxxxx kind: Secret   1  $ kubectl apply -f secret-iam.yaml   方法2. terraformで作成する 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49  resource \u0026#34;aws_iam_user\u0026#34; \u0026#34;avp\u0026#34; { name = \u0026#34;argocd-vault-plugin\u0026#34; } resource \u0026#34;aws_secretsmanager_secret\u0026#34; \u0026#34;avp\u0026#34; { name = \u0026#34;avp/test\u0026#34; } resource \u0026#34;aws_iam_user_policy\u0026#34; \u0026#34;avp\u0026#34; { name = \u0026#34;ArgocdVaultPlugin\u0026#34; user = aws_iam_user.avp.name policy = jsonencode({ \u0026#34;Version\u0026#34; : \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34; : [ { \u0026#34;Effect\u0026#34; : \u0026#34;Allow\u0026#34;, \u0026#34;Action\u0026#34; : [ \u0026#34;secretsmanager:GetResourcePolicy\u0026#34;, \u0026#34;secretsmanager:GetSecretValue\u0026#34;, \u0026#34;secretsmanager:DescribeSecret\u0026#34;, \u0026#34;secretsmanager:ListSecretVersionIds\u0026#34; ], \u0026#34;Resource\u0026#34; : [ \u0026#34;${aws_secretsmanager_secret.avp.arn}\u0026#34;, ] } ] }) } resource \u0026#34;aws_iam_access_key\u0026#34; \u0026#34;avp\u0026#34; { user = aws_iam_user.avp.name } resource \u0026#34;kubernetes_secret\u0026#34; \u0026#34;argocd-vault\u0026#34; { metadata { name = \u0026#34;argocd-vault-plugin-iam\u0026#34; namespace = \u0026#34;argo\u0026#34; } data = { \u0026#34;AWS_ACCESS_KEY_ID\u0026#34; = aws_iam_access_key.argocd-vault.id \u0026#34;AWS_SECRET_ACCESS_KEY\u0026#34; = aws_iam_access_key.argocd-vault.secret } type = \u0026#34;Opaque\u0026#34; }   2. ArgoCD with AVPのインストール 公式のインストール手順をhelmにしました。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49  ... repoServer: env: - name: AVP_TYPE value: awssecretsmanager - name: AWS_REGION value: ap-northeast-1 envFrom: - secretRef: name: argocd-vault-plugin-iam volumes: - name: custom-tools emptyDir: {} initContainers: - name: download-tools image: alpine:3.8 command: [sh, -c] args: - \u0026gt;-wget -O argocd-vault-plugin https://github.com/IBM/argocd-vault-plugin/releases/download/v1.1.1/argocd-vault-plugin_1.1.1_linux_amd64 \u0026amp;\u0026amp; chmod +x argocd-vault-plugin \u0026amp;\u0026amp; mv argocd-vault-plugin /custom-tools/ volumeMounts: - mountPath: /custom-tools name: custom-tools volumeMounts: - name: custom-tools mountPath: /usr/local/bin/argocd-vault-plugin subPath: argocd-vault-plugin server: config: configManagementPlugins: |-- name: argocd-vault-plugin generate: command: [\u0026#34;argocd-vault-plugin\u0026#34;] args: [\u0026#34;generate\u0026#34;, \u0026#34;./\u0026#34;] - name: argocd-vault-plugin-helm init: command: [sh, -c] args: [\u0026#34;helm dependency build\u0026#34;] generate: command: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;] args: [\u0026#34;helm template $ARGOCD_APP_NAME ${helm_args} . | argocd-vault-plugin generate -\u0026#34;] - name: argocd-vault-plugin-kustomize generate: command: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;] args: [\u0026#34;kustomize build . | argocd-vault-plugin generate -\u0026#34;] ...   1 2  $ helm repo add argo https://argoproj.github.io/argo-helm $ helm install argo/argo-cd -f values.yaml   3. ArgoCDにApplicationを登録 アプリケーションのデプロイ時に使用するplugin設定を定義します。 例えばアプリケーションの実装がKustomizeであれば、前項で定義した argocd-vault-plugin-kustomize　を使う事で kustomize build . | argocd-vault-plugin generate - と同様の挙動になります。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20  apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: testapp namespace: argo spec: project: default source: repoURL: \u0026#39;git@github.com:my-org/my-repo.git\u0026#39; path: path/to/yaml targetRevision: HEAD plugin: name: argocd-vault-plugin-kustomize destination: server: \u0026#39;https://kubernetes.default.svc\u0026#39; namespace: default syncPolicy: automated: prune: true selfHeal: true   あとは[テストで作成したManifest](#Kubernetes Manifestを作成する)と同様の実装でrepoにpushすればOKです。\n","permalink":"https://aaaanwz.github.io/post/2021/argocd-vault-plugin/","summary":"KubernetesでGitOps運用となると必ず話題になるのがSecretの管理です。\n Sealed Secretsやkubesecなどの手元で暗号化する系 Kubernetes Secrets Store CSI Driverやkubernetes-external-secretsなどの外部シークレットストアから引っ張ってくる系 機密情報だけ別Repoにする  など様々な方法がありますが、学習コストや実運用をイメージするとどのソリューションもしっくり来ませんでした。\nそんな中でIBM社が開発しているArgoCD Vault Pluginを触ってみたところ、ArgoCDのデプロイ時にplaceholderをreplaceするという合理的かつシンプルな仕組みで非常に好感触でした。 (上記でいう「外部シークレットストアから引っ張ってくる系」の一種に該当します) ArgoCD Vault Plugin (以下AVP) は日本語の情報が皆無に等しかったため、布教の目的も込めて導入・運用方法を記載します。\nテスト AVPはbrewからも導入でき、手元で簡単にテストができます。 シークレットストアはAWS Secrets Mangerを使う前提で解説します。\nローカル環境にインストール (Mac) 1  $ brew install argocd-vault-plugin   AWS Secrets Mangerに機密文字列を登録する 1 2  key: my_secret value: foobar   Kubernetes Manifestを作成する Secretの実装は非常に簡単で、\n アノテーションに参照するSecret Managerのパスを記述する Secret Managerのキー名を\u0026lt;\u0026gt; で囲う  だけでOKです。\n1 2 3 4 5 6 7 8  apiVersion: v1 kind: Secret metadata: name: credentials annotations: avp.","title":"ArgoCD GitOpsにおけるSecret管理"},{"content":"Argo workflowsでは Default Workflow Spec を設定する事でワークフローに色々とパッチできる。\n以下のようにexit-handlerをworkflowDefaultsにしておくと、ワークフロー側に何も記述せずとも失敗時にSlackに通知できる。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  apiVersion: v1 kind: ConfigMap metadata: name: workflow-controller-configmap data: workflowDefaults: |spec: onExit: exit-handler templates: - name: exit-handler when: \u0026#34;{{workflow.status}} != Succeeded\u0026#34; container: image: curlimages/curl:latest args: [\u0026#34;-X\u0026#34;,\u0026#34;POST\u0026#34;,\u0026#34;-H\u0026#34;,\u0026#39;Content-type: application/json\u0026#39;,\u0026#34;--data\u0026#34;, \u0026#39;{\u0026#34;attachments\u0026#34;: [{\u0026#34;title\u0026#34;:\u0026#34;Workflow status: {{workflow.status}}\u0026#34;,\u0026#34;color\u0026#34;: \u0026#34;danger\u0026#34;,\u0026#34;fields\u0026#34;: [{\u0026#34;title\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;{{workflow.name}}\u0026#34;, \u0026#34;short\u0026#34;: true }, {\u0026#34;title\u0026#34;: \u0026#34;url\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;https://{{inputs.parameters.host}}/archived-workflows/{{workflow.namespace}}/?phase=Failed\u0026#34;, \u0026#34;short\u0026#34;: false }]}]}\u0026#39;, \u0026#34;{{inputs.parameters.webhook-url}}\u0026#34;] inputs: parameters: - name: webhook-url value: https://hooks.slack.com/services/xxxx - name: host value: my-argo-workflows.example.com   もっといい実装ありそう\n","permalink":"https://aaaanwz.github.io/post/2021/argo-workflows-exit-handler/","summary":"Argo workflowsでは Default Workflow Spec を設定する事でワークフローに色々とパッチできる。\n以下のようにexit-handlerをworkflowDefaultsにしておくと、ワークフロー側に何も記述せずとも失敗時にSlackに通知できる。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  apiVersion: v1 kind: ConfigMap metadata: name: workflow-controller-configmap data: workflowDefaults: |spec: onExit: exit-handler templates: - name: exit-handler when: \u0026#34;{{workflow.status}} != Succeeded\u0026#34; container: image: curlimages/curl:latest args: [\u0026#34;-X\u0026#34;,\u0026#34;POST\u0026#34;,\u0026#34;-H\u0026#34;,\u0026#39;Content-type: application/json\u0026#39;,\u0026#34;--data\u0026#34;, \u0026#39;{\u0026#34;attachments\u0026#34;: [{\u0026#34;title\u0026#34;:\u0026#34;Workflow status: {{workflow.status}}\u0026#34;,\u0026#34;color\u0026#34;: \u0026#34;danger\u0026#34;,\u0026#34;fields\u0026#34;: [{\u0026#34;title\u0026#34;: \u0026#34;name\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;{{workflow.name}}\u0026#34;, \u0026#34;short\u0026#34;: true }, {\u0026#34;title\u0026#34;: \u0026#34;url\u0026#34;, \u0026#34;value\u0026#34;: \u0026#34;https://{{inputs.","title":"Argo Workflowsの失敗時にデフォルトでSlackに通知する"},{"content":"Argo Workflowsの公式ドキュメントが分かりづらかったので、試しにembulkを実行するテンプレートを作ってみました。\nconfig.ymlはartifactsとして渡します。\nDockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13  FROM openjdk:8-jre-alpine ARG VERSION=latest RUN mkdir -p /root/.embulk/bin \\ \u0026amp;\u0026amp; wget -q https://dl.embulk.org/embulk-${VERSION}.jar -O /root/.embulk/bin/embulk \\ \u0026amp;\u0026amp; chmod +x /root/.embulk/bin/embulk ENV PATH=$PATH:/root/.embulk/bin RUN apk add --no-cache libc6-compat RUN embulk gem install embulk-input-s3 ENTRYPOINT [\u0026#34;java\u0026#34;, \u0026#34;-jar\u0026#34;, \u0026#34;/root/.embulk/bin/embulk\u0026#34;]   1 2 3  $ EMBULK_VERSION=0.9.23 $ docker build . -t embulk:$EMBULK_VERSION --build-arg VERSION=$EMBULK_VERSION $ docker run -v /path/to/configfile:/config embulk:latest run /config/config.yml`   Workflow Template 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29  apiVersion: argoproj.io/v1alpha1 kind: WorkflowTemplate metadata: name: embulk-template spec: templates: - name: embulk container: image: embulk:latest args: [\u0026#39;run\u0026#39;, \u0026#39;/input/config.yml.liquid\u0026#39;, \u0026#39;-r\u0026#39;, \u0026#39;/output/resume-state.yml\u0026#39;] resources: limits: memory: \u0026#34;1Gi\u0026#34; cpu: \u0026#34;1\u0026#34; volumeMounts: - name: output mountPath: /output volumes: - name: output emptyDir: {} inputs: artifacts: - name: embulk-config path: /input/config.yml.liquid outputs: parameters: - name: resume-state valueFrom: path: /output/resume-state.yml   Workflow 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23  apiVersion: argoproj.io/v1alpha1 kind: Workflow metadata: name: s3-to-stdout-sample spec: entrypoint: embulk serviceAccountName: argo-workflow arguments: artifacts: - name: embulk-config raw: data: |in: type: s3 bucket: my-bucket path_prefix: path/to/file endpoint: s3-ap-northeast-1.amazonaws.com access_key_id: {{ env.AWS_ACCESS_KEY_ID }} secret_access_key: {{ env.AWS_SECRET_ACCESS_KEY }} out: type: stdout workflowTemplateRef: name: embulk-template   ","permalink":"https://aaaanwz.github.io/post/2021/argo-workflows-embulk/","summary":"Argo Workflowsの公式ドキュメントが分かりづらかったので、試しにembulkを実行するテンプレートを作ってみました。\nconfig.ymlはartifactsとして渡します。\nDockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13  FROM openjdk:8-jre-alpine ARG VERSION=latest RUN mkdir -p /root/.embulk/bin \\ \u0026amp;\u0026amp; wget -q https://dl.embulk.org/embulk-${VERSION}.jar -O /root/.embulk/bin/embulk \\ \u0026amp;\u0026amp; chmod +x /root/.embulk/bin/embulk ENV PATH=$PATH:/root/.embulk/bin RUN apk add --no-cache libc6-compat RUN embulk gem install embulk-input-s3 ENTRYPOINT [\u0026#34;java\u0026#34;, \u0026#34;-jar\u0026#34;, \u0026#34;/root/.embulk/bin/embulk\u0026#34;]   1 2 3  $ EMBULK_VERSION=0.9.23 $ docker build . -t embulk:$EMBULK_VERSION --build-arg VERSION=$EMBULK_VERSION $ docker run -v /path/to/configfile:/config embulk:latest run /config/config.","title":"embulkをArgo workflowsで実行するTemplate"},{"content":"S3にアップロードされたファイルをfluentdでBigQueryにinsertする際、S3キー名に応じてテーブルを振り分けるサンプルを掲載します。 ここではフォーマットはs3://my-bucket/{BigQueryデータセット名}/{テーブル名}/{uuid}.csv.gz とします。\nfluent.conf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46  \u0026lt;source\u0026gt; tag s3 @type s3 s3_bucket my-bucket s3_region ap-northeast-1 \u0026lt;sqs\u0026gt; queue_name my-queue \u0026lt;/sqs\u0026gt; \u0026lt;/source\u0026gt; \u0026lt;match s3\u0026gt; @type rewrite_tag_filter \u0026lt;rule\u0026gt; key s3_key pattern ^(.+?)/(.+?)/.+\\.gz$ tag bigquery.$1.$2 \u0026lt;/rule\u0026gt; \u0026lt;/match\u0026gt; \u0026lt;filter bigquery.hoge.fuga\u0026gt; @type parser key_name message \u0026lt;parse\u0026gt; @type csv keys id,family_name,first_name null_empty_string true \u0026lt;/parse\u0026gt; \u0026lt;/filter\u0026gt; \u0026lt;filter bigquery.foo.bar\u0026gt; @type parser key_name message \u0026lt;parse\u0026gt; @type csv keys column_1,column_2 null_empty_string true \u0026lt;/parse\u0026gt; \u0026lt;/filter\u0026gt; \u0026lt;match bigquery.*\u0026gt; @type bigquery_insert project my-project dataset ${tag[1]} table ${tag[2]} auth_method application_default fetch_schema true \u0026lt;buffer tag\u0026gt; @type memory \u0026lt;/buffer\u0026gt; \u0026lt;/match\u0026gt;   解説 source 1 2 3 4 5 6 7 8 9 10  \u0026lt;source\u0026gt; tag s3 @type s3 s3_bucket my-bucket s3_region ap-northeast-1 add_object_metadata true \u0026lt;sqs\u0026gt; queue_name my-queue \u0026lt;/sqs\u0026gt; \u0026lt;/source\u0026gt;   以下のファイルをアップロードしたとします。\n1  1, tanaka, taro   add_object_metadata オプションにより、以下の形にtransformされます。\n1  tag:s3 {\u0026#34;message\u0026#34;:\u0026#34;1,tanaka,taro\u0026#34;, \u0026#34;s3_bucket\u0026#34;:\u0026#34;my-bucket\u0026#34;, \u0026#34;s3_key\u0026#34;:\u0026#34;hoge/fuga/uuid.csv.gz\u0026#34;}   rewrite_tag_filter 1 2 3 4 5 6 7 8  \u0026lt;match s3\u0026gt; @type rewrite_tag_filter \u0026lt;rule\u0026gt; key s3_key pattern ^(.+?)/(.+?)/.+\\.gz$ tag bigquery.$1.$2 \u0026lt;/rule\u0026gt; \u0026lt;/match\u0026gt;   s3_keyを正規表現で展開し、タグが書き換えられます。\n1  tag:bigquery.hoge.fuga {\u0026#34;message\u0026#34;:\u0026#34;1,tanaka,taro\u0026#34;, \u0026#34;s3_bucket\u0026#34;:\u0026#34;my-bucket\u0026#34;, \u0026#34;s3_key\u0026#34;:\u0026#34;hoge/fuga/uuid.csv.gz\u0026#34;}   filter 1 2 3 4 5 6 7 8 9  \u0026lt;filter bigquery.hoge.fuga\u0026gt; @type parser key_name message \u0026lt;parse\u0026gt; @type csv keys id,family_name,first_name null_empty_string true \u0026lt;/parse\u0026gt; \u0026lt;/filter\u0026gt;   message以外のメタデータを捨て、csvデータをパースします。\n1  tag:bigquery.hoge.fuga {\u0026#34;id\u0026#34;:\u0026#34;1\u0026#34;, \u0026#34;family_name\u0026#34;:\u0026#34;tanaka\u0026#34;, \u0026#34;first_name\u0026#34;:\u0026#34;taro\u0026#34;}   bigquery_insert 1 2 3 4 5 6 7 8 9 10 11  \u0026lt;match bigquery.*\u0026gt; @type bigquery_insert project my-project dataset ${tag[1]} table ${tag[2]} auth_method application_default fetch_schema true \u0026lt;buffer tag\u0026gt; @type memory \u0026lt;/buffer\u0026gt; \u0026lt;/match\u0026gt;   タグからdatasetとtableを取得し、対象のテーブルにレコードをstreaming insertします。 tagプレースホルダを使用するためには \u0026lt;buffer tag\u0026gt; の宣言が必要になります。\nmy-project:hoge.fuga\n   id family_name first_name     1 tanaka taro    ","permalink":"https://aaaanwz.github.io/post/2021/fluentd-s3-routing/","summary":"S3にアップロードされたファイルをfluentdでBigQueryにinsertする際、S3キー名に応じてテーブルを振り分けるサンプルを掲載します。 ここではフォーマットはs3://my-bucket/{BigQueryデータセット名}/{テーブル名}/{uuid}.csv.gz とします。\nfluent.conf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46  \u0026lt;source\u0026gt; tag s3 @type s3 s3_bucket my-bucket s3_region ap-northeast-1 \u0026lt;sqs\u0026gt; queue_name my-queue \u0026lt;/sqs\u0026gt; \u0026lt;/source\u0026gt; \u0026lt;match s3\u0026gt; @type rewrite_tag_filter \u0026lt;rule\u0026gt; key s3_key pattern ^(.","title":"[fluetd] S3にアップロードされたキー名でルーティングする"},{"content":"kubectl logsはPID1の標準出力を表示するため、直接書き込んでしまえばなんでも表示できる。\n1 2 3  $ kubectl exec -it pod-xxx bash # echo \u0026#39;show as stdin\u0026#39; \u0026gt; /proc/1/fd/1 # echo \u0026#39;show as stderr\u0026#39; \u0026gt; /proc/1/fd/2   1 2 3 4  $ kubectl logs pod-xxx show as stdin show as stderr   ","permalink":"https://aaaanwz.github.io/post/2021/kubectl-logs/","summary":"kubectl logsはPID1の標準出力を表示するため、直接書き込んでしまえばなんでも表示できる。\n1 2 3  $ kubectl exec -it pod-xxx bash # echo \u0026#39;show as stdin\u0026#39; \u0026gt; /proc/1/fd/1 # echo \u0026#39;show as stderr\u0026#39; \u0026gt; /proc/1/fd/2   1 2 3 4  $ kubectl logs pod-xxx show as stdin show as stderr   ","title":"kubectl logsに任意のログを表示する"},{"content":"公式ドキュメントの Sharing Git credentials with your containerに色々と記載があるが、非常に簡単なソリューションがあったためメモ\nMac 1 2 3 4 5  $ sudo vi ~/.ssh/config Host github.com AddKeysToAgent yes UseKeychain yes   Windows 1 2 3  \u0026gt; Set-Service ssh-agent -StartupType Automatic \u0026gt; Start-Service ssh-agent \u0026gt; ssh-add $HOME/.ssh/id_rsa   ","permalink":"https://aaaanwz.github.io/post/2021/vscode-remote-container-ssh/","summary":"公式ドキュメントの Sharing Git credentials with your containerに色々と記載があるが、非常に簡単なソリューションがあったためメモ\nMac 1 2 3 4 5  $ sudo vi ~/.ssh/config Host github.com AddKeysToAgent yes UseKeychain yes   Windows 1 2 3  \u0026gt; Set-Service ssh-agent -StartupType Automatic \u0026gt; Start-Service ssh-agent \u0026gt; ssh-add $HOME/.ssh/id_rsa   ","title":"VSCode Remote ContainerからGitHubにssh接続する"},{"content":"標準的な配列の分割キーボードをようやく見つけたためAliExpressで購入。 PCBは予めソケット化されており、差し込むだけで完成なので組み立て手順は割愛。\nHHKB liteっぽくキーマップを実装。\n左スペースキー横は日英切り替え(`Alt + ``)にしてみる。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68  #include QMK_KEYBOARD_H enum layer_names { BASE, // default layer  _FN, // function layer }; enum custom_keycodes { M_KANA = SAFE_RANGE, }; bool process_record_user(uint16_t keycode, keyrecord_t *record) { if (record-\u0026gt;event.pressed) { switch(keycode) { case M_KANA: SEND_STRING(SS_LALT(\u0026#34;`\u0026#34;)); return false; } } return true; }; const uint16_t PROGMEM keymaps[][MATRIX_ROWS][MATRIX_COLS] = { /* Keymap 0: Basic layer * * ┌-----┬-----┬-----┬-----┬-----┬-----┬-----┐ ┌-----┬-----┬-----┬-----┬-----┬-----┬----------┐ * │ ~ │ 1 │ 2 │ 3 │ 4 │ 5 │ 6 │ │ 7 │ 8 │ 9 │ 0 │ - │ = │ \\ │ * ├-----┴--┬--┴--┬--┴--┬--┴--┬--┴--┬--┴--┬--┘ ┌--┴-----┴--┬--┴--┬--┴--┬--┴--┬--┴--┬--┴--┬-------┤ * │ Tab │ Q │ W │ E │ R │ T │ │ Y │ U │ I │ O │ P │ [ │ ] │ BkSp │ * ├--------┴┬----┴┬----┴┬----┴┬----┴┬----┴┐ └┬----┴┬----┴┬----┴┬----┴┬----┴┬----┴┬----┴-------┤ * │ Ctrl │ A │ S │ D │ F │ G │ │ H │ J │ K │ L │ ; │ \u0026#39; │ Enter │ * ├---------┴┬----┴┬----┴┬----┴┬----┴┬----┴┐ └┬----┴┬----┴┬----┴┬----┴┬----┴┬----┴┬-----┬-----┤ * │ LShift │ Z │ X │ C │ V │ B │ │ N │ M │ \u0026lt; │ \u0026gt; │ ? │ Sft │ Up │ Fn │ * ├------┬---┴-┬---┴--┬--┴-----┴----┬┴----┬┘ ┌┴-----┴-----┴--┬--┴----┬┴-----┼-----┼-----┼-----┤ * │ Fn │ Alt │ Win │ │ Kana│ │ │ Win │ Alt │ Lft │ Dwn │ Rgt │ * └------┴-----┴------┴-------------┴-----┘ └---------------┴-------┴------┴-----┴-----┴-----┘ */ [BASE] = LAYOUT( KC_GRV, KC_1, KC_2, KC_3, KC_4, KC_5, KC_6, KC_7, KC_8, KC_9, KC_0, KC_MINUS, KC_EQUAL, KC_BSLS, KC_TAB, KC_Q, KC_W, KC_E, KC_R, KC_T, KC_Y, KC_U, KC_I, KC_O, KC_P, KC_LBRACKET, KC_RBRACKET, KC_BSPACE, KC_LCTL, KC_A, KC_S, KC_D, KC_F, KC_G, KC_H, KC_J, KC_K, KC_L, KC_SCOLON, KC_QUOTE, KC_ENTER, KC_LSFT, KC_Z, KC_X, KC_C, KC_V, KC_B, KC_N, KC_M, KC_COMM, KC_DOT, KC_SLSH, KC_RSFT, KC_UP, MO(_FN), MO(_FN), KC_LALT, KC_LGUI, KC_SPACE, M_KANA, KC_SPACE, KC_RGUI, KC_RALT, KC_LEFT, KC_DOWN, KC_RIGHT ), /* Function * * ┌-----┬-----┬-----┬-----┬-----┬-----┬-----┐ ┌-----┬-----┬-----┬-----┬-----┬-----┬----------┐ * │ Esc │ F1 │ F2 │ F3 │ F4 │ F5 │ F6 │ │ F7 │ F8 │ F9 │ F10 │ F11 │ F12 │ Ins │ * ├-----┴--┬--┴--┬--┴--┬--┴--┬--┴--┬--┴--┬--┘ ┌--┴-----┴--┬--┴--┬--┴--┬--┴--┬--┴--┬--┴--┬-------┤ * │ Caps │ │ │ │ │ │ │ │ │ │ │ │ Up │ │ Del │ * ├--------┴┬----┴┬----┴┬----┴┬----┴┬----┴┐ └┬----┴┬----┴┬----┴┬----┴┬----┴┬----┴┬----┴-------┤ * │ │ │ │ │ │ │ │ │ │ │ │ Lft │ Rgt │ │ * ├---------┴┬----┴┬----┴┬----┴┬----┴┬----┴┐ └┬----┴┬----┴┬----┴┬----┴┬----┴┬----┴┬-----┬-----┤ * │ │ │ │ │ │ │ │ │ │ │ │ Dwn │ │ PgUp│ │ * ├------┬---┴-┬---┴--┬--┴-----┴----┬┴----┬┘ ┌┴-----┴-----┴--┬--┴----┬┴-----┼-----┼-----┼-----┤ * │ │ │ │ │ │ │ │ │ │ Home│ PgDn│ End │ * └------┴-----┴------┴-------------┴-----┘ └---------------┴-------┴------┴-----┴-----┴-----┘ */ [_FN] = LAYOUT( KC_ESC, KC_F1, KC_F2, KC_F3, KC_F4, KC_F5, KC_F6, KC_F7, KC_F8, KC_F9, KC_F10, KC_F11, KC_F12, KC_INS, KC_CAPS, _______, _______, _______, _______, _______, _______, _______, _______, _______, _______, KC_UP, _______, KC_DEL, _______, _______, _______, _______, _______, _______, _______, _______, _______, _______, KC_LEFT, KC_RIGHT, _______, _______, _______, _______, _______, _______, _______, _______, _______, _______, _______, KC_DOWN, _______, KC_PGUP, _______, _______, _______, _______, _______, _______, _______, _______, _______, KC_HOME, KC_PGDN, KC_END ), };   qmk_firmawareのドキュメントでは一番左上のキーを押しながらUSB接続でFlashモードになるとあったが、実際は一番左下のキーだった。(PCBのロットによって異なる模様)\nAliExpressにしてはお高めなだけあって、アルミ削り出しの筐体はかなり剛性感があり打ち心地は上々。\n","permalink":"https://aaaanwz.github.io/post/2021/keyboard-sp64/","summary":"標準的な配列の分割キーボードをようやく見つけたためAliExpressで購入。 PCBは予めソケット化されており、差し込むだけで完成なので組み立て手順は割愛。\nHHKB liteっぽくキーマップを実装。\n左スペースキー横は日英切り替え(`Alt + ``)にしてみる。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68  #include QMK_KEYBOARD_H enum layer_names { BASE, // default layer  _FN, // function layer }; enum custom_keycodes { M_KANA = SAFE_RANGE, }; bool process_record_user(uint16_t keycode, keyrecord_t *record) { if (record-\u0026gt;event.","title":"自作キーボードYMDK/SP64ビルドログ"},{"content":"Apache AirflowにおいてOperator間で値を渡すにはXCOMを使用しますが、\n Airflow macroで文字列として取得する PythonOperatorでtask_instanceから取得する  の2通りの方法があります。\nしかし、例えば\nGoogleCloudStorageListOperatorでファイルのリストを取得 \u0026raquo; GoogleCloudStorageToBigQueryOperator でリストされたファイルをBigQueryにロードする\nといったことをやりたい場合、XCOMからファイルのリストを配列として取得しコンストラクタに渡さなければならないためすこし工夫が必要になります。 本稿ではその実装について記載します。\nNG 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  ... list_files = GoogleCloudStorageListOperator( task_id=\u0026#39;list_files\u0026#39;, bucket=\u0026#39;my_bucket\u0026#39;, prefix=\u0026#39;path/to/file/\u0026#39;, xcom_push=True, dag=dag ) gcs_to_bigquery = GoogleCloudStorageToBigQueryOperator( task_id=\u0026#39;gcs_to_bigquery\u0026#39;, bucket=\u0026#39;my_bucket\u0026#39;, source_objects=\u0026#34;{{ ti.xcom_pull(task_ids=\u0026#39;list_files\u0026#39;) }}\u0026#34;, destination_project_dataset_table=\u0026#39;project:dataset.table\u0026#39;, autodetect=True, dag=dag ) list_files \u0026gt;\u0026gt; gcs_to_bigquery ...   ファイル名の配列がデシリアライズされた状態で source_objects に渡されてしまうため動作しません。\nOK 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35  ... class CustomGcsToBigQueryOperator(GoogleCloudStorageToBigQueryOperator): def __init__(self, *args, **kwargs): super().__init__( source_objects=[], *args, **kwargs ) self.source_objects_task_id = kwargs[\u0026#39;source_objects_task_id\u0026#39;] def execute(self, context): self.source_objects=context[\u0026#39;ti\u0026#39;].xcom_pull(task_ids=self.source_objects_task_id) super().execute(context) list_files = GoogleCloudStorageListOperator( task_id=\u0026#39;list_files\u0026#39;, bucket=\u0026#39;my_bucket\u0026#39;, prefix=\u0026#39;path/to/file/\u0026#39;, xcom_push=True, dag=dag ) gcs_to_bigquery = CustomGcsToBigQueryOperator( task_id=\u0026#39;gcs_to_bigquery\u0026#39;, bucket=\u0026#39;my_bucket\u0026#39;, source_objects_task_id=\u0026#39;list_files\u0026#39;, destination_project_dataset_table=\u0026#39;project:dataset.table\u0026#39;, autodetect=True, dag=dag ) list_files \u0026gt;\u0026gt; gcs_to_bigquery ...   GoogleCloudStorageToBigQueryOperator を継承したカスタムオペレーターを実装し、execute で xcom_pull すればOKです。\n","permalink":"https://aaaanwz.github.io/post/2021/airflow-xcom/","summary":"Apache AirflowにおいてOperator間で値を渡すにはXCOMを使用しますが、\n Airflow macroで文字列として取得する PythonOperatorでtask_instanceから取得する  の2通りの方法があります。\nしかし、例えば\nGoogleCloudStorageListOperatorでファイルのリストを取得 \u0026raquo; GoogleCloudStorageToBigQueryOperator でリストされたファイルをBigQueryにロードする\nといったことをやりたい場合、XCOMからファイルのリストを配列として取得しコンストラクタに渡さなければならないためすこし工夫が必要になります。 本稿ではその実装について記載します。\nNG 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  ... list_files = GoogleCloudStorageListOperator( task_id=\u0026#39;list_files\u0026#39;, bucket=\u0026#39;my_bucket\u0026#39;, prefix=\u0026#39;path/to/file/\u0026#39;, xcom_push=True, dag=dag ) gcs_to_bigquery = GoogleCloudStorageToBigQueryOperator( task_id=\u0026#39;gcs_to_bigquery\u0026#39;, bucket=\u0026#39;my_bucket\u0026#39;, source_objects=\u0026#34;{{ ti.xcom_pull(task_ids=\u0026#39;list_files\u0026#39;) }}\u0026#34;, destination_project_dataset_table=\u0026#39;project:dataset.table\u0026#39;, autodetect=True, dag=dag ) list_files \u0026gt;\u0026gt; gcs_to_bigquery ...   ファイル名の配列がデシリアライズされた状態で source_objects に渡されてしまうため動作しません。","title":"Airflowで後続のOperatorに配列を渡す"},{"content":"1  kubectl create job 作成するJob名 --from=cronjob/CronJob名   https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-job-em-\n","permalink":"https://aaaanwz.github.io/post/2020/k8s-cron-rerun/","summary":"1  kubectl create job 作成するJob名 --from=cronjob/CronJob名   https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#-em-job-em-","title":"KubernetesのCronJobからJobを手動作成する"},{"content":"terraformに対応していないクラウドリソースを local-exec を用いてterraform化してみます。 今回はBigQueryのユーザー定義関数(UDF)でやってみます。\n実装 さて早速。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  variable project{} resource \u0026#34;null_resource\u0026#34; \u0026#34;bigquery-udf\u0026#34; { \u0026lt;-#1  triggers = { query = \u0026#34;CREATE OR REPLACE FUNCTION my_dataset.TEST_FUNCTION(x INT64) AS (x + 1);\u0026#34; \u0026lt;-#2  } provisioner \u0026#34;local-exec\u0026#34; { \u0026lt;-#3  interpreter = [\u0026#34;bq\u0026#34;, \u0026#34;query\u0026#34;, \u0026#34;--use_legacy_sql=false\u0026#34;, \u0026#34;--project_id=${var.project}\u0026#34;] \u0026lt;-#4  command = self.triggers.query on_failure = fail \u0026lt;-#5  } provisioner \u0026#34;local-exec\u0026#34; { when = destroy \u0026lt;-#6  interpreter = [\u0026#34;bq\u0026#34;, \u0026#34;query\u0026#34;, \u0026#34;--use_legacy_sql=false\u0026#34;, \u0026#34;--project_id=${var.project}\u0026#34;] command = \u0026#34;DROP FUNCTION IF EXISTS ${element(regex(\u0026#34;FUNCTION\\\\s(.+?)[\\\\s\\\\(]\u0026#34;, file(\u0026#34;${path.module}/udf/${each.value}\u0026#34;)), 0)}\u0026#34; \u0026lt;-#7  on_failure = fail } }   解説 #1 執筆時点では google provderはBigQueryのUDFに対応していないため、 null_resource として定義します。 https://www.terraform.io/docs/provisioners/null_resource.html\n#2 リソースの本体です。この部分が変更されるとresource updateとして扱われます。 https://registry.terraform.io/providers/hashicorp/null/latest/docs/resources/resource#argument-reference\n#3 リソースのcreate時の挙動をlocal-exec provisionerとして定義します。 https://www.terraform.io/docs/provisioners/local-exec.html\n#4 実際に実行するコマンドを定義します。 今回の例では command にSQL自身を、interpreter にはSQLを実行するための bq コマンドを記述します。 https://www.terraform.io/docs/provisioners/local-exec.html#argument-reference\n#5 コマンドの失敗時の挙動 (terraformコマンド自体を失敗させるかどうか) を設定します。 https://www.terraform.io/docs/provisioners/index.html#failure-behavior\n#6 when = destroyを宣言するとリソース削除時に実行するコマンドを定義できます。 https://www.terraform.io/docs/provisioners/index.html#destroy-time-provisioners\n#7 terraformとは関係ありませんが、リソース削除時はDDLから関数名を正規表現で抽出してDROPするようにしてみました。\n 複数のUDFを管理する場合はprovisionerの定義が冗長になるので、queryをvariableに定義してfor_each を使う実装になるかと思います。 CI/CDパイプライン等のterraformコマンドの実行環境にbqコマンドを用意しなければならないデメリットがあるため、リソースをterraform管理にするメリットがそのコストを上回ると判断される場合には有用かと思います。\n","permalink":"https://aaaanwz.github.io/post/2020/terraform-local-exec/","summary":"terraformに対応していないクラウドリソースを local-exec を用いてterraform化してみます。 今回はBigQueryのユーザー定義関数(UDF)でやってみます。\n実装 さて早速。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  variable project{} resource \u0026#34;null_resource\u0026#34; \u0026#34;bigquery-udf\u0026#34; { \u0026lt;-#1  triggers = { query = \u0026#34;CREATE OR REPLACE FUNCTION my_dataset.TEST_FUNCTION(x INT64) AS (x + 1);\u0026#34; \u0026lt;-#2  } provisioner \u0026#34;local-exec\u0026#34; { \u0026lt;-#3  interpreter = [\u0026#34;bq\u0026#34;, \u0026#34;query\u0026#34;, \u0026#34;--use_legacy_sql=false\u0026#34;, \u0026#34;--project_id=${var.project}\u0026#34;] \u0026lt;-#4  command = self.triggers.query on_failure = fail \u0026lt;-#5  } provisioner \u0026#34;local-exec\u0026#34; { when = destroy \u0026lt;-#6  interpreter = [\u0026#34;bq\u0026#34;, \u0026#34;query\u0026#34;, \u0026#34;--use_legacy_sql=false\u0026#34;, \u0026#34;--project_id=${var.","title":"terraform非対応リソースをlocal-execで管理する"},{"content":"1 2 3 4  $ gcloud projects add-iam-policy-binding myproject --member=serviceAccount:myserviceaccount@myproject.iam.gserviceaccount.com --role=\u0026#39;roles/mycustomrole\u0026#39; ERROR: Policy modification failed. For a binding with condition, run \u0026#34;gcloud alpha iam policies lint-condition\u0026#34; to identify issues in condition. ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/mycustomrole is not supported for this resource.   --role の指定を roles/mycustomrole ではなく projects/myproject/roles/mycustomroleにすればOK\n1 2 3  $ gcloud projects add-iam-policy-binding myproject --member=serviceAccount:myserviceaccuont@myproject.iam.gserviceaccount.com --role=\u0026#39;projects/myproject/roles/mycustomrole\u0026#39; Updated IAM policy for project [myproject].   ","permalink":"https://aaaanwz.github.io/post/2020/gcp-serviceaccount-error/","summary":"1 2 3 4  $ gcloud projects add-iam-policy-binding myproject --member=serviceAccount:myserviceaccount@myproject.iam.gserviceaccount.com --role=\u0026#39;roles/mycustomrole\u0026#39; ERROR: Policy modification failed. For a binding with condition, run \u0026#34;gcloud alpha iam policies lint-condition\u0026#34; to identify issues in condition. ERROR: (gcloud.projects.add-iam-policy-binding) INVALID_ARGUMENT: Role roles/mycustomrole is not supported for this resource.   --role の指定を roles/mycustomrole ではなく projects/myproject/roles/mycustomroleにすればOK\n1 2 3  $ gcloud projects add-iam-policy-binding myproject --member=serviceAccount:myserviceaccuont@myproject.iam.gserviceaccount.com --role=\u0026#39;projects/myproject/roles/mycustomrole\u0026#39; Updated IAM policy for project [myproject].   ","title":"GCPでカスタムロールをサービスアカウントにbindingしようとしてエラーになる場合"},{"content":"HHKB Professionalを終のキーボードにしようと思っていましたが、\n 長時間コーディングをしていると小指が痛くなる なんだかんだ矢印キーは欲しい  という事で、最近流行りの自作キーボードをやってみようかという運びになりました。\nパーツ購入  1 ~ = が上段に並んでいて欲しい 小指で押していたShift、Enter、Deleteを親指付近に移動させたい  という理由によりサイズは12列5行に決定、このサイズのPCBを探したところAliExpressで JJ50 という製品を見つけました。\n同時にステンレス製ケース、キーキャップ、CherryMX銀軸を購入。\n総額は¥14800(PCB¥3700、ケース¥4100、キーキャップ¥3600、スイッチ¥3400)程度でした。\n説明書なんてものは当然ありません。ドキュメントの類はこれが全て。\n組み立て   ケースにスイッチを取り付ける\n  PCBを載せて半田付け\n  キーキャップを取り付けてハードウェアは完成\n  表面実装部品は予めついていたため拍子抜けするほど簡単でした。\nファームウェア焼き ビルド環境セットアップ (Mac) 1 2 3 4  $ git clone https://github.com/qmk/qmk_firmware $ cd qmk_firmware $ util/qmk_install.sh $ make git-submodule   キーマップの作成 マクロや３つ以上のレイヤーは使う予定が無いので、とてもシンプルなコードです。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54  #include QMK_KEYBOARD_H #define ______ KC_TRNS #define _DEFLT 0 #define _FN 1  bool process_record_user(uint16_t keycode, keyrecord_t *record) { return true; }; const uint16_t PROGMEM keymaps[][MATRIX_ROWS][MATRIX_COLS] = { /* Default * ,-----------------------------------------------------------------------------------. * | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 | 0 | - | = | * |------+------+------+------+------+------+------+------+------+------+------+------| * | Q | W | E | R | T | Y | U | I | O | P | [ | ] | * |------+------+------+------+------+------+------+------+------+------+------+------| * | A | S | D | F | G | H | J | K | L | ; | \u0026#34; | ` | * |------+------+------+------+------+------+------+------+------+------+------+------| * | Z | X | C | V | B | N | M | , | . | / | Up | \\ | * |------+------+------+------+------+------+------+------+------+------+------+------| * | Fn | Ctrl | Alt | GUI | Space| Enter| Shift| Bksp | Tab | Left | Down | Right| * `-----------------------------------------------------------------------------------\u0026#39; */ [_DEFLT] = LAYOUT( \\ KC_1, KC_2, KC_3, KC_4, KC_5, KC_6, KC_7, KC_8, KC_9, KC_0, KC_MINS, KC_EQL, \\ KC_Q, KC_W, KC_E, KC_R, KC_T, KC_Y, KC_U, KC_I, KC_O, KC_P, KC_LBRC, KC_RBRC, \\ KC_A, KC_S, KC_D, KC_F, KC_G, KC_H, KC_J, KC_K, KC_L, KC_SCLN, KC_QUOT, KC_GRV, \\ KC_Z, KC_X, KC_C, KC_V, KC_B, KC_N, KC_M, KC_COMM, KC_DOT, KC_SLSH, KC_UP , KC_BSLS, \\ MO(_FN), KC_LCTL, KC_LALT, KC_LGUI, KC_SPC, KC_ENT, KC_LSFT, KC_BSPC, KC_TAB, KC_LEFT, KC_DOWN, KC_RGHT \\ ), /* Fn * ,-----------------------------------------------------------------------------------. * | F1 | F2 | F3 | F4 | F5 | F6 | F7 | F8 | F9 | F10 | F11 | F12 | * |------+------+------+------+------+------+------+------+------+------+------+------| * | | | Esc | | | | | | | | | | * |------+------+------+------+------+------+------+------+------+------+------+------| * | | | | | | | | | | | | | * |------+------+------+------+------+------+------+------+------+------+------+------| * | | | | | | | | | | | PgUp | | * |------+------+------+------+------+------+------+------+------+------+------+------| * | | | | | | | |Delete| | Home | PgDn | End | * `-----------------------------------------------------------------------------------\u0026#39; */ [_FN] = LAYOUT( \\ KC_F1, KC_F2, KC_F3, KC_F4, KC_F5, KC_F6, KC_F7, KC_F8, KC_F9, KC_F10, KC_F11, KC_F12, \\ _______, _______, KC_ESC, _______, _______, _______, _______, _______, _______, _______, _______, _______, \\ _______, _______, _______, _______, _______, _______, _______, _______, _______, _______, _______, _______, \\ _______, _______, _______, _______, _______, _______, _______, _______, _______, _______, KC_PGUP, _______, \\ _______, _______, _______, _______, _______, _______, _______, KC_DEL, _______, KC_HOME, KC_PGDN, KC_END \\ ) };   クローズドな金属ケースのため、LEDを無効にします。\n1 2 3 4  ... BACKLIGHT_ENABLE = no RGBLIGHT_ENABLE = no ...   ファームウェア焼き  最も右上の一個下のキー(このキーボードの場合は ]) を押しながらPCにUSB接続します。  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33  $ make jj50:mykeyboard:flash QMK Firmware 0.7.163 Making jj50 with keymap mykeyboard and target flash avr-gcc (GCC) 8.3.0 Copyright (C) 2018 Free Software Foundation, Inc. This is free software; see the source for copying conditions. There is NO warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. Size before: text data bss dec hex filename 0 15208 0 15208 3b68 .build/jj50_mykeyboard.hex Compiling: tmk_core/common/command.c [OK] Linking: .build/jj50_mykeyboard.elf [OK] Creating load file for flashing: .build/jj50_mykeyboard.hex [OK] Copying jj50_mykeyboard.hex to qmk_firmware folder [OK] Checking file size of jj50_mykeyboard.hex [OK] * The firmware size is fine - 15208/28672 (53%, 13464 bytes free) Warning: could not detach kernel HID driver: Function not implemented Warning: could not detach kernel HID driver: Function not implemented Warning: could not detach kernel HID driver: Function not implemented Warning: could not detach kernel HID driver: Function not implemented Warning: could not detach kernel HID driver: Function not implemented Warning: could not detach kernel HID driver: Function not implemented Page size = 128 (0x80) Device size = 32768 (0x8000); 30720 bytes remaining Uploading 15232 (0x3b80) bytes starting at 0 (0x0) 0x00000 ... 0x00080Page size = 128 (0x80) Device size = 32768 (0x8000); 30720 bytes remaining Uploading 15232 (0x3b80) bytes starting at 0 (0x0) 0x03b00 ... 0x03b80 0x03b00 ... 0x03b80   QMKの公式では QMK toolboxというツールが推奨されていますが、私の環境ではキーボードを認識しませんでした。\n感想  作業時間は2時間くらい、特に問題もなく完了。 キーマップ以前に格子配列に慣れが必要。この記事を書くのに相当時間かかった。 JJ50は手頃な価格でサイズも過不足なくお勧め。同じKPrepublicが販売しているキーキャップも良い品質。ケースは精度・仕上げに難ありで他で買った方が良い。  ","permalink":"https://aaaanwz.github.io/post/2020/keyboard-jj50/","summary":"HHKB Professionalを終のキーボードにしようと思っていましたが、\n 長時間コーディングをしていると小指が痛くなる なんだかんだ矢印キーは欲しい  という事で、最近流行りの自作キーボードをやってみようかという運びになりました。\nパーツ購入  1 ~ = が上段に並んでいて欲しい 小指で押していたShift、Enter、Deleteを親指付近に移動させたい  という理由によりサイズは12列5行に決定、このサイズのPCBを探したところAliExpressで JJ50 という製品を見つけました。\n同時にステンレス製ケース、キーキャップ、CherryMX銀軸を購入。\n総額は¥14800(PCB¥3700、ケース¥4100、キーキャップ¥3600、スイッチ¥3400)程度でした。\n説明書なんてものは当然ありません。ドキュメントの類はこれが全て。\n組み立て   ケースにスイッチを取り付ける\n  PCBを載せて半田付け\n  キーキャップを取り付けてハードウェアは完成\n  表面実装部品は予めついていたため拍子抜けするほど簡単でした。\nファームウェア焼き ビルド環境セットアップ (Mac) 1 2 3 4  $ git clone https://github.com/qmk/qmk_firmware $ cd qmk_firmware $ util/qmk_install.sh $ make git-submodule   キーマップの作成 マクロや３つ以上のレイヤーは使う予定が無いので、とてもシンプルなコードです。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54  #include QMK_KEYBOARD_H #define ______ KC_TRNS #define _DEFLT 0 #define _FN 1  bool process_record_user(uint16_t keycode, keyrecord_t *record) { return true; }; const uint16_t PROGMEM keymaps[][MATRIX_ROWS][MATRIX_COLS] = { /* Default * ,-----------------------------------------------------------------------------------.","title":"自作キーボード JJ50 ビルドログ"},{"content":"オペミスやat-least-onceセマンティクスによってINSERTされてしまった重複レコードを消すSQLです。\n完全に同一な重複レコードを消す やる事は\n 重複レコードのうち最古のものを一時テーブルに退避 重複レコードを全て削除 一時テーブルから再度INSERT です。  Schema\n1 2 3 4 5 6 7 8 9 10 11 12  [ { \u0026#34;mode\u0026#34;: \u0026#34;REQUIRED\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;id\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;INTEGER\u0026#34; }, { \u0026#34;mode\u0026#34;: \u0026#34;NULLABLE\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;value\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;STRING\u0026#34; } ]   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  CREATE TABLE project_name.tmp OPTIONS( expiration_timestamp = TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 1 HOUR) ) AS( SELECT id, value FROM ( SELECT *, COUNT(id) OVER(PARTITION BY id) AS count, ROW_NUMBER() OVER(PARTITION BY id) AS row_number FROM project_name.table_name ) WHERE count \u0026gt; 1 AND row_number = 1 ); DELETE FROM project_name.table_name WHERE id IN( SELECT id FROM project_name.tmp ); INSERT INTO project_name.table_name( SELECT * FROM project_name.tmp ); DROP TABLE project_name.tmp;   残すレコードが最古のものでなくても構わないのであれば、最初のCREATE は少し単純化できます。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  CREATE TABLE project_name.tmp OPTIONS( expiration_timestamp = TIMESTAMP_ADD(CURRENT_TIMESTAMP(), INTERVAL 1 HOUR) ) AS( SELECT id, value FROM ( SELECT *, ROW_NUMBER() OVER(PARTITION BY id) AS row_number FROM project_name.table ) WHERE row_number = 2 );    一部が異なるレコードを消す 同一 idで timestamp が違うレコードが存在する場合、最も古いレコード以外を削除するクエリです。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  MERGE dataset_name.table_name table USING ( SELECT id, MIN(timestamp) AS timestamp FROM dataset_name.table_name GROUP BY id HAVING COUNT(id) \u0026gt; 1 ) duplicated_origin ON table.id = duplicated_origin.id AND table.timestamp != duplicated_origin.timestamp WHEN MATCHED THEN DELETE ;   ","permalink":"https://aaaanwz.github.io/post/2020/bigquery-deduplication/","summary":"オペミスやat-least-onceセマンティクスによってINSERTされてしまった重複レコードを消すSQLです。\n完全に同一な重複レコードを消す やる事は\n 重複レコードのうち最古のものを一時テーブルに退避 重複レコードを全て削除 一時テーブルから再度INSERT です。  Schema\n1 2 3 4 5 6 7 8 9 10 11 12  [ { \u0026#34;mode\u0026#34;: \u0026#34;REQUIRED\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;id\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;INTEGER\u0026#34; }, { \u0026#34;mode\u0026#34;: \u0026#34;NULLABLE\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;value\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;STRING\u0026#34; } ]   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39  CREATE TABLE project_name.","title":"BigQueryで重複レコードを削除するSQL"},{"content":"Apache beamのJava quickstartがいまいち分かりづらかったため、最小コードとデプロイ手順(Google Cloud Dataflow, AWS EMR)を備忘録としてまとめる\nWordCountサンプル https://github.com/aaaanwz/beam-wordcount-sample\n1 2 3 4 5 6 7 8 9 10  . ├── pom.xml └── src └── main └── java ├── core │ └── WordCount.java └── dafn ├── ExtractWordsFn.java └── FormatAsTextFn.java   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105  \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;aaaanwz\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;beam-wordcount-sample\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.1\u0026lt;/version\u0026gt; \u0026lt;packaging\u0026gt;jar\u0026lt;/packaging\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;maven.compiler.source\u0026gt;1.8\u0026lt;/maven.compiler.source\u0026gt; \u0026lt;maven.compiler.target\u0026gt;1.8\u0026lt;/maven.compiler.target\u0026gt; \u0026lt;beam.version\u0026gt;2.16.0\u0026lt;/beam.version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;profiles\u0026gt; \u0026lt;profile\u0026gt; \u0026lt;id\u0026gt;direct-runner\u0026lt;/id\u0026gt; \u0026lt;activation\u0026gt; \u0026lt;activeByDefault\u0026gt;true\u0026lt;/activeByDefault\u0026gt; \u0026lt;/activation\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.beam\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;beam-runners-direct-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${beam.version}\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/profile\u0026gt; \u0026lt;profile\u0026gt; \u0026lt;id\u0026gt;dataflow-runner\u0026lt;/id\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.beam\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;beam-runners-google-cloud-dataflow-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${beam.version}\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/profile\u0026gt; \u0026lt;profile\u0026gt; \u0026lt;id\u0026gt;flink-runner\u0026lt;/id\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.beam\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;beam-runners-flink-1.9\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${beam.version}\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;runtime\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;/profile\u0026gt; \u0026lt;/profiles\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.beam\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;beam-sdks-java-core\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;${beam.version}\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.slf4j\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;slf4j-simple\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.7.30\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-shade-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;createDependencyReducedPom\u0026gt;false\u0026lt;/createDependencyReducedPom\u0026gt; \u0026lt;filters\u0026gt; \u0026lt;filter\u0026gt; \u0026lt;artifact\u0026gt;*:*\u0026lt;/artifact\u0026gt; \u0026lt;excludes\u0026gt; \u0026lt;exclude\u0026gt;META-INF/*.SF\u0026lt;/exclude\u0026gt; \u0026lt;exclude\u0026gt;META-INF/*.DSA\u0026lt;/exclude\u0026gt; \u0026lt;exclude\u0026gt;META-INF/*.RSA\u0026lt;/exclude\u0026gt; \u0026lt;/excludes\u0026gt; \u0026lt;/filter\u0026gt; \u0026lt;/filters\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;shade\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;shadedArtifactAttached\u0026gt;true\u0026lt;/shadedArtifactAttached\u0026gt; \u0026lt;shadedClassifierName\u0026gt;shaded\u0026lt;/shadedClassifierName\u0026gt; \u0026lt;transformers\u0026gt; \u0026lt;transformer implementation=\u0026#34;org.apache.maven.plugins.shade.resource.ServicesResourceTransformer\u0026#34;/\u0026gt; \u0026lt;transformer implementation=\u0026#34;org.apache.maven.plugins.shade.resource.ManifestResourceTransformer\u0026#34;\u0026gt; \u0026lt;mainClass\u0026gt;core.WordCount\u0026lt;/mainClass\u0026gt; \u0026lt;/transformer\u0026gt; \u0026lt;/transformers\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt;   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28  package core; import org.apache.beam.sdk.Pipeline; import org.apache.beam.sdk.coders.StringUtf8Coder; import org.apache.beam.sdk.io.TextIO; import org.apache.beam.sdk.options.PipelineOptions; import org.apache.beam.sdk.options.PipelineOptionsFactory; import org.apache.beam.sdk.transforms.Count; import org.apache.beam.sdk.transforms.Create; import org.apache.beam.sdk.transforms.MapElements; import org.apache.beam.sdk.transforms.ParDo; import dafn.ExtractWordsFn; import dafn.FormatAsTextFn; public class WordCount { public static void main(String[] args) { PipelineOptions options = PipelineOptionsFactory.fromArgs(args).withValidation().create(); Pipeline p = Pipeline.create(options); p.apply(\u0026#34;ReadLines\u0026#34;, Create.of(\u0026#34;a a a a a b b b b b c c c \u0026#34;).withCoder(StringUtf8Coder.of())) .apply(ParDo.of(new ExtractWordsFn())).apply(Count.perElement()) .apply(MapElements.via(new FormatAsTextFn())) .apply(\u0026#34;WriteCounts\u0026#34;, TextIO.write().to(\u0026#34;counts\u0026#34;)); p.run().waitUntilFinish(); } }   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  package dafn; import org.apache.beam.sdk.metrics.Counter; import org.apache.beam.sdk.metrics.Distribution; import org.apache.beam.sdk.metrics.Metrics; import org.apache.beam.sdk.transforms.DoFn; public class ExtractWordsFn extends DoFn\u0026lt;String, String\u0026gt; { private static final long serialVersionUID = 1L; private final Counter emptyLines = Metrics.counter(ExtractWordsFn.class, \u0026#34;emptyLines\u0026#34;); private final Distribution lineLenDist = Metrics.distribution(ExtractWordsFn.class, \u0026#34;lineLenDistro\u0026#34;); @ProcessElement public void processElement(@Element String element, OutputReceiver\u0026lt;String\u0026gt; receiver) { lineLenDist.update(element.length()); if (element.trim().isEmpty()) { emptyLines.inc(); } String[] words = element.split(\u0026#34;[^\\\\p{L}]+\u0026#34;, -1); for (String word : words) { if (!word.isEmpty()) { receiver.output(word); } } } }   1 2 3 4 5 6 7 8 9 10 11 12 13 14  package dafn; import org.apache.beam.sdk.transforms.SimpleFunction; import org.apache.beam.sdk.values.KV; public class FormatAsTextFn extends SimpleFunction\u0026lt;KV\u0026lt;String, Long\u0026gt;, String\u0026gt; { private static final long serialVersionUID = 1L; @Override public String apply(KV\u0026lt;String, Long\u0026gt; input) { return input.getKey() + \u0026#34;: \u0026#34; + input.getValue(); } }   デプロイ方法 ローカル実行 1 2  mvn package java -jar ./target/beam-wordcount-sample-0.1-shaded.jar   Google Cloud Dataflow  gcloud CLIのインストール・設定済みであることが前提  1 2  mvn package -Pdataflow-runner java -jar ./target/beam-wordcount-sample-0.1-shaded.jar --runner=DataflowRunner --project=xxxx --tempLocation=gs://\u0026lt;YOUR_GCS_BUCKET\u0026gt;/temp/   AWS EMR  aws cliのインストール・設定済みであることが前提 Flink を使用してクラスターを作成する  1 2  mvn package -Pflink-runner scp -i ~/.ssh/keypair.pem ./target/beam-wordcount-sample-0.1-shaded.jar ec2-user@ec2-xxx-xxx-xxx:/home/hadoop   1 2 3 4  JAR の場所:command-runner.jar メインクラス:なし 引数:flink run -m yarn-cluster -yn 2 /home/hadoop/beam-wordcount-sample-0.1-shaded.jar --runner=FlinkRunner 失敗時の操作:次へ   (参考)\n","permalink":"https://aaaanwz.github.io/post/2020/apache-beam-example/","summary":"Apache beamのJava quickstartがいまいち分かりづらかったため、最小コードとデプロイ手順(Google Cloud Dataflow, AWS EMR)を備忘録としてまとめる\nWordCountサンプル https://github.com/aaaanwz/beam-wordcount-sample\n1 2 3 4 5 6 7 8 9 10  . ├── pom.xml └── src └── main └── java ├── core │ └── WordCount.java └── dafn ├── ExtractWordsFn.java └── FormatAsTextFn.java   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105  \u0026lt;?","title":"Apache beam サンプルコード"},{"content":" ssh-keygenコマンドで公開鍵/秘密鍵を生成する 公開鍵(id_rsa.pub)をGitHubのDeploy keyに登録する 秘密鍵(id_rsa)をCircleCIに登録する 3.のfingerprintを↓にコピー  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  version: 2.1 jobs: do-something-with-another-repository: docker: - image: circleci/golang:1.11-stretch steps: - add_ssh_keys: fingerprints: - \u0026#34;aa:bb:cc:dd:ee:ff:gg:hh:ii:jj:kk:ll:mm:nn:oo:pp\u0026#34; - run: GIT_SSH_COMMAND=\u0026#34;ssh -o StrictHostKeyChecking=no\u0026#34; git clone git@github.com:aaaanwz/another-repository.git - run: echo \u0026#39;Do something\u0026#39; workflows: test: jobs: - do-something-with-another-repository   ","permalink":"https://aaaanwz.github.io/post/2019/circleci-deploy-key/","summary":" ssh-keygenコマンドで公開鍵/秘密鍵を生成する 公開鍵(id_rsa.pub)をGitHubのDeploy keyに登録する 秘密鍵(id_rsa)をCircleCIに登録する 3.のfingerprintを↓にコピー  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  version: 2.1 jobs: do-something-with-another-repository: docker: - image: circleci/golang:1.11-stretch steps: - add_ssh_keys: fingerprints: - \u0026#34;aa:bb:cc:dd:ee:ff:gg:hh:ii:jj:kk:ll:mm:nn:oo:pp\u0026#34; - run: GIT_SSH_COMMAND=\u0026#34;ssh -o StrictHostKeyChecking=no\u0026#34; git clone git@github.com:aaaanwz/another-repository.git - run: echo \u0026#39;Do something\u0026#39; workflows: test: jobs: - do-something-with-another-repository   ","title":"CircleCIでDeploy Keyを用いて別のprivate repoをcloneする"},{"content":"GitHubでreleaseが作成された時、Lambdaにコードを反映させバージョンを更新するワークフローの単純な実装です\nサンプルディレクトリ構成 1 2 3 4 5 6 7  some-lambda-function-repo ├── .github │ └── workflows │ └── lambda-cd.yml ├── README.md ├── bootstrap └── handler.sh   GitHubのSecretsに以下を設定  AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY  必要なPolicyに関しては割愛します\nGithub Actions 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  name: Lambda Continuous Delivery on: push: tags: - \u0026#39;*\u0026#39; jobs: lambda-cd: runs-on: ubuntu-latest steps: - uses: actions/checkout@master - run: chmod u+x * - run: zip -r /tmp/some-lambda-function.zip * - uses: actions/setup-python@v1 with: python-version: 3.7 - run: pip3 install awscli - run: aws lambda update-function-code --function-name some-lambda-function --zip-file fileb:///tmp/some-lambda-function.zip --publish env: AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }} AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }} AWS_DEFAULT_REGION: ap-northeast-1   ","permalink":"https://aaaanwz.github.io/post/2019/github-actions-lambda-deploy/","summary":"GitHubでreleaseが作成された時、Lambdaにコードを反映させバージョンを更新するワークフローの単純な実装です\nサンプルディレクトリ構成 1 2 3 4 5 6 7  some-lambda-function-repo ├── .github │ └── workflows │ └── lambda-cd.yml ├── README.md ├── bootstrap └── handler.sh   GitHubのSecretsに以下を設定  AWS_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY  必要なPolicyに関しては割愛します\nGithub Actions 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  name: Lambda Continuous Delivery on: push: tags: - \u0026#39;*\u0026#39; jobs: lambda-cd: runs-on: ubuntu-latest steps: - uses: actions/checkout@master - run: chmod u+x * - run: zip -r /tmp/some-lambda-function.","title":"Github ActionsでAWS Lambdaにデプロイする"},{"content":"git branchに変更が加わった際、\n JUnit test (with MySQL) docker build Kubernetes環境にデプロイ (CIOps)  が行われるJavaプロジェクトを構築します。\n 本番運用ではArgoCDなどgitOps構築をお勧めします\n 登場するもの OSS  Maven MySQL Docker  サービス  GitHub CircleCI AWS (ECR, EKS) ⇦ 微修正でその他マネージドk8sにも応用可能かと思います。  サンプルプロジェクトの実装 最終的にディレクトリ構成はこんな感じになります。順を追って作っていきます。 GitHubからcloneして頂いても結構です。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  testproject/ ├ src/ │ ├ main/ │ │ └ java/ │ │ └testpackage/ │ │ └Main.java │ └ test/ │ └ java/ │ └testpackage/ │ └MainTest.java ├ .cifcleci/ │ └ config.yml ├ schema.sql ├ pom.xml ├ deploy.yaml └ Dockerfile   1. スキーマ定義 テーブルスキーマを記述します。resourceフォルダに置いても良いのですが、今回はトップディレクトリに配置することにします。\nデータベース名は指定しません。\n1  CREATE TABLE user (id int, name varchar(10));   2. mavenプロジェクト作成 先ほどのtableに適当なレコードを挿入する最小限のJavaプロジェクトを実装します。\nJDBC、JUnit、maven-assembly-pluginを使用します。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59  \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;aaaanwz\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;test\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.1\u0026lt;/version\u0026gt; \u0026lt;name\u0026gt;testproject\u0026lt;/name\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;maven.compiler.source\u0026gt;11\u0026lt;/maven.compiler.source\u0026gt; \u0026lt;maven.compiler.target\u0026gt;11\u0026lt;/maven.compiler.target\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;!-- JDBC driver --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;mysql\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;mysql-connector-java\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;8.0.17\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;!-- JUnit --\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.junit.jupiter\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit-jupiter-engine\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.3.1\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;!-- JUnit test --\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;maven-surefire-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.0.0-M3\u0026lt;/version\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;!-- Build runnable jar --\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;artifactId\u0026gt;maven-assembly-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;executions\u0026gt; \u0026lt;execution\u0026gt; \u0026lt;phase\u0026gt;package\u0026lt;/phase\u0026gt; \u0026lt;goals\u0026gt; \u0026lt;goal\u0026gt;single\u0026lt;/goal\u0026gt; \u0026lt;/goals\u0026gt; \u0026lt;/execution\u0026gt; \u0026lt;/executions\u0026gt; \u0026lt;configuration\u0026gt; \u0026lt;archive\u0026gt; \u0026lt;manifest\u0026gt; \u0026lt;mainClass\u0026gt;testpackage.Main\u0026lt;/mainClass\u0026gt; \u0026lt;/manifest\u0026gt; \u0026lt;/archive\u0026gt; \u0026lt;descriptorRefs\u0026gt; \u0026lt;descriptorRef\u0026gt;jar-with-dependencies\u0026lt;/descriptorRef\u0026gt; \u0026lt;/descriptorRefs\u0026gt; \u0026lt;/configuration\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt;   1レコード挿入するだけのMainクラスを実装します。\nデータベースへの接続情報は環境変数から取得します。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  package testpackage; import java.sql.Connection; import java.sql.DriverManager; import java.sql.PreparedStatement; import java.sql.SQLException; public class Main { public static void main(String[] args) throws ClassNotFoundException, SQLException { final String host = System.getenv(\u0026#34;DB_HOST\u0026#34;); final String dbname = System.getenv(\u0026#34;DB_NAME\u0026#34;); execute(host, dbname); } static void execute(String host, String dbname) throws ClassNotFoundException, SQLException { Class.forName(\u0026#34;com.mysql.cj.jdbc.Driver\u0026#34;); Connection conn = DriverManager.getConnection( \u0026#34;jdbc:mySql://\u0026#34; + host + \u0026#34;/\u0026#34; + dbname + \u0026#34;?useSSL=false\u0026#34;, \u0026#34;root\u0026#34;, \u0026#34;\u0026#34;); PreparedStatement stmt = conn.prepareStatement(\u0026#34;INSERT INTO user(id,name) VALUES(?, ?)\u0026#34;); stmt.setInt(1, 1); stmt.setString(2, \u0026#34;Yamada\u0026#34;); stmt.executeUpdate(); } }   レコードが挿入された事を確認するだけのテストを書きます。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46  package testpackage; import static org.junit.jupiter.api.Assertions.assertEquals; import static org.junit.jupiter.api.Assertions.fail; import java.sql.Connection; import java.sql.DriverManager; import java.sql.ResultSet; import java.sql.SQLException; import java.sql.Statement; import org.junit.jupiter.api.AfterEach; import org.junit.jupiter.api.BeforeEach; import org.junit.jupiter.api.Test; import testpackage.Main; class MainTest { Statement stmt; @BeforeEach void before() throws ClassNotFoundException, SQLException { Class.forName(\u0026#34;com.mysql.cj.jdbc.Driver\u0026#34;); Connection conn = DriverManager.getConnection(\u0026#34;jdbc:mySql://localhost/test?useSSL=false\u0026#34;, \u0026#34;root\u0026#34;, \u0026#34;\u0026#34;); stmt = conn.createStatement(); } @AfterEach void after() throws SQLException { stmt.executeUpdate(\u0026#34;TRUNCATE TABLE user\u0026#34;); } @Test void test() throws Exception { Main.execute(\u0026#34;localhost\u0026#34;, \u0026#34;test\u0026#34;); try (ResultSet rs = stmt.executeQuery(\u0026#34;SELECT * FROM user WHERE id = 1;\u0026#34;)) { if (rs.next()) { assertEquals(\u0026#34;Yamada\u0026#34;, rs.getString(\u0026#34;name\u0026#34;)); } else { fail(); } } } }   3. Dockerfile作成 Dockerfileを書きます。テストは docker buildとは別のステップで行う予定のため、-DskipTestsを付与してビルド時のテストをスキップします。\nイメージサイズ削減のためmaven:3.6でビルド、openjdk11:apline-slimで実行するマルチステージビルドを行います。 maven:3.6では約800MB、openjdk11:alpine-slimでは約300MBとなります。現時点でECRの無料枠は500MBのため、個人開発では大きな差です。\n1 2 3 4 5 6 7 8 9 10  FROM maven:3.6 AS build ADD . /var/tmp/testproject/ WORKDIR /var/tmp/testproject/ RUN mvn -DskipTests package FROM adoptopenjdk/openjdk11:alpine-slim COPY --from=build /var/tmp/testproject/target/test-0.0.1-jar-with-dependencies.jar /usr/local/ CMD java -jar /usr/local/test-0.0.1-jar-with-dependencies.jar.jar   4. k8s yamlファイル作成 3.でビルドされたコンテナをk8sにデプロイするためのyamlを書きます。今回のプログラムは単発でSQLを実行して終了するため、kind: Jobにしてみます。Docker registryのurlは適宜置換してください。\nenvでDBへの接続情報を定義します。 DB_HOSTの値が mysqlとなっているのは、KubernetesのService経由で接続するためです。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  apiVersion: batch/v1 kind: Job metadata: name: test spec: template: spec: containers: - name: test image: your-docker-registry-url/testimage:latest imagePullPolicy: Always env: - name: DB_HOST value: \u0026#34;mysql\u0026#34; - name: DB_NAME value: \u0026#34;ekstest\u0026#34; restartPolicy: Never backoffLimit: 0   5. circleci configファイル作成 本稿のキモです。CircleCIで自動テスト/ビルドを行うための構成設定を書きます。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61  version: 2.1 orbs: aws-ecr: circleci/aws-ecr@6.1.0 aws-eks: circleci/aws-eks@0.2.1 kubernetes: circleci/kubernetes@0.3.0 jobs: test: # ①JUnit テストの実行 docker: - image: circleci/openjdk:11 - image: circleci/mysql:5.7 environment: MYSQL_ALLOW_EMPTY_PASSWORD: yes MYSQL_DATABASE: test command: [--character-set-server=utf8, --collation-server=utf8_general_ci, --default-storage-engine=innodb] steps: - checkout - run: name: Waiting for MySQL to be ready command: dockerize -wait tcp://localhost:3306 -timeout 1m - run: name: Install MySQL CLI; Create table; command: sudo apt-get install default-mysql-client \u0026amp;\u0026amp; mysql -h 127.0.0.1 -uroot test \u0026lt; schema.sql - restore_cache: key: circleci-test-{{ checksum \u0026#34;pom.xml\u0026#34; }} - run: mvn dependency:go-offline - save_cache: paths: - ~/.m2 key: circleci-test-{{ checksum \u0026#34;pom.xml\u0026#34; }} - run: mvn test - store_test_results: path: target/surefire-reports deploy: # ③EKSへのkubectl apply executor: aws-eks/python3 steps: - checkout - kubernetes/install - aws-eks/update-kubeconfig-with-authenticator: cluster-name: test-cluster aws-region: \u0026#34;${AWS_REGION}\u0026#34; - run: command: | kubectl apply -f k8s-job.yaml name: apply workflows: test-and-deploy: jobs: - test: # ①JUnit テストの実行 filters: branches: only: develop - aws-ecr/build-and-push-image: # ②コンテナのビルドとECRへのpush filters: branches: only: master create-repo: true repo: \u0026#34;testimage\u0026#34; tag: \u0026#34;latest\u0026#34; - deploy: # ③EKSへのkubectl apply requires: - aws-ecr/build-and-push-image   ①JUnitテストの実行 filterによって、developブランチに変更が加わった際に mvn testが実行されるようにしてみました。この際テスト用MySQLインスタンスが立ち上がり、 testデータベースが準備されます。\n公式ドキュメントで詳しく解説されています。　 Language Guide: Java Database Configuration Examples  ②コンテナのビルドとECRへのpush masterブランチに変更が加わった際に、Dockerfileに従ってbuildし、ECRにpushします。\nOrbクイックスタートガイドで詳しく解説されています。\n③EKSへのkubectl apply ②が実施された後、4.で定義したJobを実行します。\ncircleci/aws-eksに解説がありますが、サンプルコードをよりシンプルに変更しました。\n@0.2.1のドキュメントによるとパラメータaws-regionはRequiredとなっていませんが、実際は必須のようです。ECRのpushで環境変数AWS_REGIONを要求されているので、これをそのまま使いました。\nインフラ設定 ほとんど公式ドキュメントへのリンク集です。\n1. ECR, EKS環境準備 1-1. 基本設定 test-clusterを立ち上げます。\n Getting Started with Amazon EKS Setting Up with Amazon ECR  1-2. EKS上にMySQLをデプロイ Kubernetes公式にDeploy MySQLというドキュメントが用意されています。\nkubectl exec -it mysql-0 mysqlコマンドからekstestデータベースとuserテーブルを用意します。\n2. CircleCI projectの設定 2-1. プロジェクトのセットアップ  quick-start  2-2. 環境変数の設定  circleci/aws-ecrでRequiredとなっている環境変数をCircleCI Projectに設定します。\nRoleに要求される権限の詳細は割愛します。  まとめ  develop branchにpush\nmvn testが実行され、テストレポートがCircleCIのTest summaryに表示されます。 master branchにpush\nECRにdocker imageがpushされ、EKSでJobが開始します。EKS上のMySQLに id:1 name:Yamadaレコードが挿入されます。  ","permalink":"https://aaaanwz.github.io/post/2019/java-create-maven-project/","summary":"git branchに変更が加わった際、\n JUnit test (with MySQL) docker build Kubernetes環境にデプロイ (CIOps)  が行われるJavaプロジェクトを構築します。\n 本番運用ではArgoCDなどgitOps構築をお勧めします\n 登場するもの OSS  Maven MySQL Docker  サービス  GitHub CircleCI AWS (ECR, EKS) ⇦ 微修正でその他マネージドk8sにも応用可能かと思います。  サンプルプロジェクトの実装 最終的にディレクトリ構成はこんな感じになります。順を追って作っていきます。 GitHubからcloneして頂いても結構です。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  testproject/ ├ src/ │ ├ main/ │ │ └ java/ │ │ └testpackage/ │ │ └Main.java │ └ test/ │ └ java/ │ └testpackage/ │ └MainTest.","title":"mavenプロジェクト作成からCIOps構築まで"},{"content":"Javaプロセスを一定時間毎にチェックし、ハングしていればPodを再起動する仕組みの備忘録です。\nKubernetes LivenessProbeに関する詳細はこちらをご参照ください。\nJava実装 監視対象クラス テスト用に、インスタンスが生成されてから10秒後に isAlive() == falseになるように実装します。\n1 2 3 4 5 6 7 8 9 10  public class SomeResource { final long createdTime; public SomeResource() { this.createdTime = System.currentTimeMillis(); } public boolean isAlive() { return System.currentTimeMillis() - createdTime \u0026lt; 10000; } }   監視用エンドポイント SomeResource#isAlive() == trueの時はレスポンスコード 200, falseの時は 500を返すように実装します。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  import com.sun.net.httpserver.HttpServer; import java.io.IOException; import java.net.InetSocketAddress; public class HeartBeat { private final SomeResource target; private HttpServer httpServer; private final int port; public HeartBeat(SomeResource target, int port) { this.target = target; this.port = port; } void start() throws IOException { httpServer = HttpServer.create(new InetSocketAddress(port), 0); httpServer.createContext(\u0026#34;/heartbeat\u0026#34;, httpExchange -\u0026gt; { int responseCode; if (target.isAlive()) { responseCode = 200; } else { responseCode = 500; } byte[] responseBody = String.valueOf(target.isAlive()).getBytes(StandardCharsets.UTF_8); httpExchange.getResponseHeaders().add(\u0026#34;Content-Type\u0026#34;, \u0026#34;text/plain; charset=UTF-8\u0026#34;); httpExchange.sendResponseHeaders(responseCode, responseBody.length); httpExchange.getResponseBody().write(responseBody); httpExchange.close(); }); httpServer.start(); } void stop() { httpServer.stop(0); } }   Mainクラス 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  import java.util.concurrent.CountDownLatch; public class Main { public static void main(String[] args) { final CountDownLatch latch = new CountDownLatch(1); SomeResource resource = new SomeResource(); HeartBeat heartbeat = new HeartBeat(resource, 1234);// ポート1234番を使用  Runtime.getRuntime().addShutdownHook(new Thread() { @Override public void run() { heartbeat.stop(); latch.countDown(); } }); try { heartbeat.start(); latch.await(); } catch (Exception e) { e.printStackTrace(); System.exit(-1); } } }   curlでエンドポイントをテストする SomeClassインスタンス生成直後 1 2 3 4 5 6 7  $ curl --dump-header - http://localhost:1234/heartbeat HTTP/1.1 200 OK Date: Tue, 02 Jul 2019 07:24:20 GMT Content-type: text/plain; charset=UTF-8 Content-length: 4 true   10秒後 1 2 3 4 5 6 7  $ curl --dump-header - http://localhost:1234/heartbeat HTTP/1.1 500 Internal Server Error Date: Tue, 02 Jul 2019 07:24:30 GMT Content-type: text/plain; charset=UTF-8 Content-length: 5 false   Kubernetesにデプロイ コンテナの作成、レジストリへのpush手順は省略します。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26  apiVersion: apps/v1 kind: Deployment metadata: name: java-resource-watch-test spec: replicas: 1 selector: matchLabels: app: java-resource-watch-test template: metadata: labels: app: java-resource-watch-test spec: containers: - name: java-resource-watch-test image: some_registry/test:latest livenessProbe: httpGet: path: /heartbeat port: 8080 httpHeaders: - name: Custom-Header value: HealthCheck initialDelaySeconds: 1 periodSeconds: 1   kubectl apply -f ./deployment.yaml\nSomeResourceの寿命が切れると同時にPodが終了し、再起動される事が確認できるかと思います。\n","permalink":"https://aaaanwz.github.io/post/2019/java-k8s-liveness-probe/","summary":"Javaプロセスを一定時間毎にチェックし、ハングしていればPodを再起動する仕組みの備忘録です。\nKubernetes LivenessProbeに関する詳細はこちらをご参照ください。\nJava実装 監視対象クラス テスト用に、インスタンスが生成されてから10秒後に isAlive() == falseになるように実装します。\n1 2 3 4 5 6 7 8 9 10  public class SomeResource { final long createdTime; public SomeResource() { this.createdTime = System.currentTimeMillis(); } public boolean isAlive() { return System.currentTimeMillis() - createdTime \u0026lt; 10000; } }   監視用エンドポイント SomeResource#isAlive() == trueの時はレスポンスコード 200, falseの時は 500を返すように実装します。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37  import com.","title":"Kubernetes Liveness ProbeでJavaプロセスを監視する"},{"content":"juniversalchardetを使用して、\n ファイルの文字コードを推測・デコード・コンソールへの表示を行う URLエンコードされた文字列をデコードする  の2つのサンプルプログラムを作成してみます。\njuniversalchardetとはMozillaによって提供されているライブラリで、バイト列のパターンの出現頻度をもとに文字コードを推測する機能を提供します。現在日本語ではISO-2022-JP, SHIFT-JIS, EUC-JPに対応しています。\n開発環境  OpenJDK 11 Maven 3.6  下準備 以下をmaven dependenciesに追加します\n1 2 3 4 5  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.googlecode.juniversalchardet\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;juniversalchardet\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;   サンプル1. ファイル読み込み Detectorクラス 今回は汎用性のためにInputStreamを引数としてみます。 引数に渡されたInputStreamインスタンスはオフセットが進んでしまう事に注意が必要です。 UniversalDetectorは入力データが全てシングルバイト文字の場合は文字コード判定結果がnullとなります。今回はそのような場合は環境デフォルト値を返すようにしました。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  import java.io.IOException; import java.io.InputStream; import java.nio.charset.Charset; import org.mozilla.universalchardet.UniversalDetector; public class Detector { public static Charset getCharsetName(InputStream is) throws IOException { //4kbのメモリバッファを確保する  byte[] buf = new byte[4096]; UniversalDetector detector = new UniversalDetector(null); //文字コードの推測結果が得られるまでInputStreamを読み進める  int nread; while ((nread = is.read(buf)) \u0026gt; 0 \u0026amp;\u0026amp; !detector.isDone()) { detector.handleData(buf, 0, nread); } //推測結果を取得する  detector.dataEnd(); final String detectedCharset = detector.getDetectedCharset(); detector.reset(); if (detectedCharset != null) { return Charset.forName(detectedCharset); } //文字コードを取得できなかった場合、環境のデフォルトを使用する  return Charset.forName(System.getProperty(\u0026#34;file.encoding\u0026#34;)); } }   Mainクラス ファイルの文字コードを判別し、コンソールに出力します。\nFileInputStreamはmark/resetをサポートしていないため、文字コード判別とコンソール出力で別のインスタンスを生成します。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  import java.io.BufferedReader; import java.io.FileInputStream; import java.io.IOException; import java.io.InputStreamReader; import java.nio.charset.Charset; public class Main { public static void main(String[] args) throws IOException { final String path = \u0026#34;./test.txt\u0026#34;; Charset cs; try (FileInputStream fis = new FileInputStream(path)) { cs = Detector.getCharsetName(fis); System.out.println(\u0026#34;charset:\u0026#34; + cs); } try (BufferedReader br =new BufferedReader(new InputStreamReader(new FileInputStream(path), cs))) { br.lines().forEach(s -\u0026gt; System.out.println(s)); } } }   実行例 1 2  charset:SHIFT-JIS あいうえお   サンプル2. URLのデコード 追加でApache commons codecを使用します。\n1 2 3 4 5  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;commons-codec\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;commons-codec\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.12\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;   Detectorクラス 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21  import java.io.IOException; import java.io.InputStream; import java.nio.charset.Charset; import org.mozilla.universalchardet.UniversalDetector; public class Detector { public static Charset getCharsetName(byte[] bytes) { UniversalDetector detector = new UniversalDetector(null); //入力文字列が短すぎると推測ができないため、入力を繰り返す while (!detector.isDone()) { detector.handleData(bytes, 0, bytes.length); detector.dataEnd(); } final String charsetName = detector.getDetectedCharset(); detector.reset(); if (charsetName != null) { return Charset.forName(charsetName); } return Charset.forName(System.getProperty(\u0026#34;file.encoding\u0026#34;)); } }   Mainクラス 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  import java.io.UnsupportedEncodingException; import java.nio.charset.Charset; import java.nio.charset.StandardCharsets; import org.apache.commons.codec.DecoderException; import org.apache.commons.codec.net.URLCodec; public class Main { public static void main(String[] args) throws DecoderException, UnsupportedEncodingException { final String str= \u0026#34;%82%a0%82%a2%82%a4%82%a6%82%a8\u0026#34;; //URLエンコード文字列をパースし、バイト配列にする byte[] bytes = new URLCodec() .decode(str, StandardCharsets.ISO_8859_1.name()) .getBytes(StandardCharsets.ISO_8859_1.name()); Charset cs = Detector.getCharsetName(bytes); System.out.println(\u0026#34;charset:\u0026#34;+cs); //バイト配列を検出したcharsetを用いて文字列にする final String s = new String(bytes,cs); System.out.println(s); } }   実行例 1 2  charset:SHIFT-JIS あいうえお   ","permalink":"https://aaaanwz.github.io/post/2019/juniversalchardet/","summary":"juniversalchardetを使用して、\n ファイルの文字コードを推測・デコード・コンソールへの表示を行う URLエンコードされた文字列をデコードする  の2つのサンプルプログラムを作成してみます。\njuniversalchardetとはMozillaによって提供されているライブラリで、バイト列のパターンの出現頻度をもとに文字コードを推測する機能を提供します。現在日本語ではISO-2022-JP, SHIFT-JIS, EUC-JPに対応しています。\n開発環境  OpenJDK 11 Maven 3.6  下準備 以下をmaven dependenciesに追加します\n1 2 3 4 5  \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;com.googlecode.juniversalchardet\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;juniversalchardet\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;1.0.3\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt;   サンプル1. ファイル読み込み Detectorクラス 今回は汎用性のためにInputStreamを引数としてみます。 引数に渡されたInputStreamインスタンスはオフセットが進んでしまう事に注意が必要です。 UniversalDetectorは入力データが全てシングルバイト文字の場合は文字コード判定結果がnullとなります。今回はそのような場合は環境デフォルト値を返すようにしました。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30  import java.","title":"Javaで文字コードを推測する"},{"content":"Kafka Streams DSLのうち、ステートフルな操作(join,reduce,aggregate,windowingなど)を実際に触り、動作を確認します。\nまた最後に、本稿と前回で登場した関数を使用してステートフルなストリームFizzBuzzを実装してみます。\n実際にやってみる 前々回の記事(準備編)のプロジェクトが作成済みである事を前提とします。\nKTable まずはじめに、KTable,KGroupedStreamについて知っておく必要があります。 KGroupedStreamはkeyの値毎にグループ化されたKStreamで、KTableはkeyとvalueの最新状態を保持するテーブルとして扱えるものです。\nKTableはnew StreamsBuilder().table(\u0026quot;topic-name\u0026quot;)...のように直接トピックから生成したり、KGroupedStreamを集約して生成したりと様々なルートで生成することができます。\n公式ドキュメントの以下の図が非常に分かりやすいです。\n画像リンク元ページ\nAggregate KGroupedStreamをkeyごとに集約し、KTableに変換します。 コードと実行結果を見るのが一番早いと思います。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  private static Initializer\u0026lt;String\u0026gt; initializer = () -\u0026gt; \u0026#34;InitVal\u0026#34;; private static Aggregator\u0026lt;String, String, String\u0026gt; aggregator = (key, val, agg) -\u0026gt; agg + \u0026#34; \u0026amp; \u0026#34; + val; public static Topology getTopology() { StreamsBuilder builder = new StreamsBuilder(); builder .stream(\u0026#34;input-topic\u0026#34;) .groupByKey() .aggregate(initializer, aggregator) .toStream() // KTableの更新履歴をストリームとして取り出す .to(\u0026#34;output-topic\u0026#34;); return builder.build(); }   Initializerは初期値を返す関数で、Java streamで言うとSupplierです。\nAggregatorはkey,value,現在のステートの3つを引数として受け取り、新たなステートを生成するための関数です。 aggregateの結果はKTableですが、更新結果を確認するためにtoStream()を使います。\n1 2 3 4 5 6 7 8 9 10 11 12  @Test void test() throws InterruptedException { inputRecord(\u0026#34;input-topic\u0026#34;, \u0026#34;key1\u0026#34;, \u0026#34;hoge\u0026#34;); inputRecord(\u0026#34;input-topic\u0026#34;, \u0026#34;key1\u0026#34;, \u0026#34;fuga\u0026#34;); inputRecord(\u0026#34;input-topic\u0026#34;, \u0026#34;key2\u0026#34;, \u0026#34;foo\u0026#34;); inputRecord(\u0026#34;input-topic\u0026#34;, \u0026#34;key2\u0026#34;, \u0026#34;bar\u0026#34;); System.out.println(getOutputRecord(\u0026#34;output-topic\u0026#34;)); System.out.println(getOutputRecord(\u0026#34;output-topic\u0026#34;)); System.out.println(getOutputRecord(\u0026#34;output-topic\u0026#34;)); System.out.println(getOutputRecord(\u0026#34;output-topic\u0026#34;)); }   1 2 3 4  topic=output-topic, key=key1, value=InitVal \u0026amp; hoge topic=output-topic, key=key1, value=InitVal \u0026amp; hoge \u0026amp; fuga topic=output-topic, key=key2, value=InitVal \u0026amp; foo topic=output-topic, key=key2, value=InitVal \u0026amp; foo \u0026amp; bar   keyに対応するvalueが、aggregatorに記述した通りにどんどん連結されていっていますね。\n簡易版のような機能としてreduceとcountも用意されています。 reduceはinitializerが無いaggregateのようなもので、特定のkeyに対して最初に来たvalueが初期値になります。\n1 2 3 4 5 6 7 8 9 10 11 12  private static Reducer\u0026lt;String\u0026gt; reducer = (val, agg) -\u0026gt; agg + \u0026#34; \u0026amp; \u0026#34; + val; ... builder .stream(\u0026#34;input-topic\u0026#34;) .groupByKey() .reduce(reducer) .toStream() .to(\u0026#34;output-topic\u0026#34;); ...   1 2  key=key1, value=hoge key=key1, value=hoge \u0026amp; fuga   count 簡易版aggregateで、keyに対するレコード数をカウントするKTableを生成します。\n元のレコード内容は失われてしまうので、throughなどで別のストリームを生やして使うのが一般的かと思います。\n1 2 3 4 5 6 7 8 9 10 11 12  public static Topology getTopology() { StreamsBuilder builder = new StreamsBuilder(); builder .stream(\u0026#34;input-topic\u0026#34;); .groupByKey() .count() .toStream() .to(\u0026#34;output-topic\u0026#34;, Produced.with(Serdes.String(), Serdes.Long())); return builder.build(); }   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  private ProducerRecord\u0026lt;String, Long\u0026gt; getOutputRecord(String topicName) { return testDriver.readOutput(topicName, Serdes.String().deserializer(), Serdes.Long().deserializer()); } @Test void test() throws InterruptedException { inputRecord(\u0026#34;input-topic\u0026#34;, \u0026#34;key1\u0026#34;, \u0026#34;hoge\u0026#34;); inputRecord(\u0026#34;input-topic\u0026#34;, \u0026#34;key1\u0026#34;, \u0026#34;fuga\u0026#34;); inputRecord(\u0026#34;input-topic\u0026#34;, \u0026#34;key2\u0026#34;, \u0026#34;foo\u0026#34;); inputRecord(\u0026#34;input-topic\u0026#34;, \u0026#34;key2\u0026#34;, \u0026#34;bar\u0026#34;); System.out.println(getOutputRecord(\u0026#34;output-topic\u0026#34;)); System.out.println(getOutputRecord(\u0026#34;output-topic\u0026#34;)); System.out.println(getOutputRecord(\u0026#34;output-topic\u0026#34;)); System.out.println(getOutputRecord(\u0026#34;output-topic\u0026#34;)); }   1 2 3 4  topic=output-topic, key=key1, value=1 topic=output-topic, key=key1, value=2 topic=output-topic, key=key2, value=1 topic=output-topic, key=key2, value=2   Windowing ストリームを時間枠で切り取ります。Tubling, Hopping, Sliding, Sessionの4種類がありここでは紹介しきれないため、公式ドキュメントの図を見ていただくのが最もわかりやすいかと思います。\n今回は1000msのTumblingでレコードをcountしてみます。 windowedByを行うと、Windowedオブジェクトがkeyとして使用されます。 試しに.toString()でkeyの内容も確認してみましょう\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  public static Topology getTopology() { StreamsBuilder builder = new StreamsBuilder(); builder .stream(\u0026#34;input-topic\u0026#34;) .groupByKey() .windowedBy(TimeWindows.of(Duration.ofMillis(1000))) .count() .toStream() .selectKey((key, value) -\u0026gt; key.toString()) .to(\u0026#34;output-topic\u0026#34;, Produced.with(Serdes.String(), Serdes.Long())); return builder.build(); }   1 2 3 4 5 6 7 8 9 10 11 12 13  @Test void test() throws InterruptedException { inputRecord(\u0026#34;input-topic\u0026#34;, \u0026#34;key1\u0026#34;, \u0026#34;hoge\u0026#34;); inputRecord(\u0026#34;input-topic\u0026#34;, \u0026#34;key1\u0026#34;, \u0026#34;fuga\u0026#34;); inputRecord(\u0026#34;input-topic\u0026#34;, \u0026#34;key1\u0026#34;, \u0026#34;foo\u0026#34;); Thread.sleep(1000); inputRecord(\u0026#34;input-topic\u0026#34;, \u0026#34;key1\u0026#34;, \u0026#34;bar\u0026#34;); System.out.println(getOutputRecord(\u0026#34;output-topic\u0026#34;)); System.out.println(getOutputRecord(\u0026#34;output-topic\u0026#34;)); System.out.println(getOutputRecord(\u0026#34;output-topic\u0026#34;)); System.out.println(getOutputRecord(\u0026#34;output-topic\u0026#34;)); }   1 2 3 4  topic=output-topic, key=[key1@1559886985000/1559886986000], value=1 topic=output-topic, key=[key1@1559886985000/1559886986000], value=2 topic=output-topic, key=[key1@1559886985000/1559886986000], value=3 topic=output-topic, key=[key1@1559886986000/1559886987000], value=1   keyが{元のkey値}@{window開始時刻}/{window終了時刻}になっていますね。 そしてhoge``fuga``fooは同一window内でカウントされ、時間が開いたbarは次のwindowに入っています。\nJoin 同一のkeyを持つ2つのレコードを組み合わせ、新たなストリームを生成します。 KStream同士、KStreamとKTable、KTable同士それぞれで実行できます。 例としてKStream同士のjoinを行ってみます。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  private static ValueJoiner\u0026lt;String, String, String\u0026gt; joiner = (left, right) -\u0026gt; left + \u0026#34; \u0026amp; \u0026#34; + right; public static Topology getTopology() { StreamsBuilder builder = new StreamsBuilder(); KStream\u0026lt;String, String\u0026gt; streamA = builder.stream(\u0026#34;input-topicA\u0026#34;); KStream\u0026lt;String, String\u0026gt; streamB = builder.stream(\u0026#34;input-topicB\u0026#34;); streamA .join(streamB, joiner, JoinWindows.of(Duration.ofMillis(100))) .to(\u0026#34;output-topic\u0026#34;); return builder.build(); }   ValueJoinerは2つのvalueから新たなvalueを生成する関数です。\nKStream同士をjoinする場合は、JoinWindowsで2つのレコードが揃うまでの待ち受け時間を定義します。相手がKTableの場合はkeyに対する最後(最新)のvalueの値がjoinされるため、Windowの定義は不要です。\n1 2 3 4 5 6 7 8 9 10 11  @Test void test() throws InterruptedException { inputRecord(\u0026#34;input-topicA\u0026#34;, \u0026#34;key1\u0026#34;, \u0026#34;hoge\u0026#34;); inputRecord(\u0026#34;input-topicB\u0026#34;, \u0026#34;key1\u0026#34;, \u0026#34;fuga\u0026#34;); inputRecord(\u0026#34;input-topicA\u0026#34;, \u0026#34;key2\u0026#34;, \u0026#34;foo\u0026#34;); Thread.sleep(200); inputRecord(\u0026#34;input-topicB\u0026#34;, \u0026#34;key2\u0026#34;, \u0026#34;bar\u0026#34;); System.out.println(getOutputRecord(\u0026#34;output-topic\u0026#34;)); System.out.println(getOutputRecord(\u0026#34;output-topic\u0026#34;)); }   1 2  topic=output-topic, key=key1, value=hoge \u0026amp; fuga null   key1を持つhogeとfugaが結合しています。また、fooとbarはJoinWindow内に収まっていないため、joinが実施されていません。\njoinの他にleftJoinとouterJoinが用意されています。KSQL同様、リレーショナルDBの感覚ですね。\nステートフルFizzBuzzを実装する さて、長くなりましたがKafka Streams DSLの大方を触ってみました。\nこれまで見てきた物を使って、ステートフルなFizzBuzzを実装してみましょう。 以下の要件でやってみます。\n 自然数をconsumeする度に加算していき、総和が3の倍数ならFizz, 5の倍数ならBuzz, 15の倍数ならFizzBuzz、それ以外なら現在の総和をoutput-topicにproduceする。 FizzBuzzが出力されるか、5秒以上入力レコードが来なければ総和を0に戻す  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32  private static Aggregator\u0026lt;String, Integer, String\u0026gt; aggregator = (k, v, a) -\u0026gt; { Integer sum; try { sum = Integer.valueOf(a); } catch (NumberFormatException e) { sum = 0; } sum += v; if (sum % 15 == 0) { return \u0026#34;FizzBuzz (\u0026#34; + sum + \u0026#34;)\u0026#34;; } else if (sum % 3 == 0) { return \u0026#34;Fizz (\u0026#34; + sum + \u0026#34;)\u0026#34;; } else if (sum % 5 == 0) { return \u0026#34;Buzz (\u0026#34; + sum + \u0026#34;)\u0026#34;; } return sum.toString(); }; public static Topology getTopology() { StreamsBuilder builder = new StreamsBuilder(); builder .stream(\u0026#34;input-topic\u0026#34;, Consumed.with(Serdes.String(), Serdes.Integer())) .groupByKey(Grouped.with(Serdes.String(), Serdes.Integer())) .windowedBy(TimeWindows.of(Duration.ofMillis(5000))) .aggregate(() -\u0026gt; \u0026#34;0\u0026#34;, aggregator) .toStream() .selectKey((k, v) -\u0026gt; k.toString()) .to(\u0026#34;output-topic\u0026#34;, Produced.with(Serdes.String(), Serdes.String())); return builder.build(); }   あまり美しくはないですが、最も単純に「前回の計算結果をそのままステートにする」実装にしてみました。 是非実際のKafka環境をセットアップして遊んでみてください。\nConsume/Publishで実装するとちょっと面倒な要件も、非常にシンプルに記述できることが実感できたかと思います。 これで完結となります、ご覧いただきありがとうございました！\n","permalink":"https://aaaanwz.github.io/post/2019/kafkastreams-3/","summary":"Kafka Streams DSLのうち、ステートフルな操作(join,reduce,aggregate,windowingなど)を実際に触り、動作を確認します。\nまた最後に、本稿と前回で登場した関数を使用してステートフルなストリームFizzBuzzを実装してみます。\n実際にやってみる 前々回の記事(準備編)のプロジェクトが作成済みである事を前提とします。\nKTable まずはじめに、KTable,KGroupedStreamについて知っておく必要があります。 KGroupedStreamはkeyの値毎にグループ化されたKStreamで、KTableはkeyとvalueの最新状態を保持するテーブルとして扱えるものです。\nKTableはnew StreamsBuilder().table(\u0026quot;topic-name\u0026quot;)...のように直接トピックから生成したり、KGroupedStreamを集約して生成したりと様々なルートで生成することができます。\n公式ドキュメントの以下の図が非常に分かりやすいです。\n画像リンク元ページ\nAggregate KGroupedStreamをkeyごとに集約し、KTableに変換します。 コードと実行結果を見るのが一番早いと思います。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17  private static Initializer\u0026lt;String\u0026gt; initializer = () -\u0026gt; \u0026#34;InitVal\u0026#34;; private static Aggregator\u0026lt;String, String, String\u0026gt; aggregator = (key, val, agg) -\u0026gt; agg + \u0026#34; \u0026amp; \u0026#34; + val; public static Topology getTopology() { StreamsBuilder builder = new StreamsBuilder(); builder .","title":"Kafka Streams DSLを一通り体験する (3. ステートフル処理実践編)"},{"content":"Kafka Streams DSLのうち、ステートレスな操作(branch,map,mergeなど)を実際に触り、動作を確認します。 また最後に、本稿で登場する関数を使用してストリーム処理のFizzBuzzを実装してみます。\n前回の記事(準備編)のプロジェクトが作成済みである事を前提とします。\n実際にやってみる Filter Java StreamのByPredicateと同じと思って差し支えありません。Java StreamのPredicateと紛らわしいのでimport対象に注意しましょう。\nkey, valueを引数にbooleanを返し、falseの場合はレコードが除外されます。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  import org.apache.kafka.streams.kstream.Predicate; ... private static Predicate\u0026lt;String, String\u0026gt; predicate = (key, value) -\u0026gt; value.startsWith(\u0026#34;あ\u0026#34;); public static Topology getTopology() { StreamsBuilder builder = new StreamsBuilder(); builder .stream(\u0026#34;input-topic\u0026#34;, Consumed.with(Serdes.String(), Serdes.String())) //使用するデシリアライザにStringを明示指定します .filter(predicate) .to(\u0026#34;output-topic\u0026#34;); return builder.build(); }   ここでConsumed.with(...)が新たに登場しました。Predicateの引数がString型なので、デシリアライザも明示指定する必要があるためです。\nまた.filter((key, value)-\u0026gt; value.startsWith(\u0026quot;あ\u0026quot;))のように直接ラムダ式を記述することももちろん可能です。\nさて、テストを実行してみましょう。\n1 2 3 4 5 6 7  @Test void test() { inputRecord(\u0026#34;input-topic\u0026#34;, \u0026#34;key1\u0026#34;, \u0026#34;あけまして\u0026#34;); System.out.println(getOutputRecord(\u0026#34;output-topic\u0026#34;)); inputRecord(\u0026#34;input-topic\u0026#34;, \u0026#34;key1\u0026#34;, \u0026#34;おめでとう\u0026#34;); System.out.println(getOutputRecord(\u0026#34;output-topic\u0026#34;)); }   1 2  topic=output-topic, key=key1, value=あけまして null   \u0026ldquo;あ\u0026quot;から始まるvalueを持つレコードだけがfilterを通過することが確認できました。\nBranch Java関数型には今の所存在しない機能です。 複数のkstream.Predicateを引数に持ち、ストリームを分岐させることができます。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16  private static Predicate\u0026lt;String, String\u0026gt; predicateA = (key, value) -\u0026gt; value.startsWith(\u0026#34;あ\u0026#34;); private static Predicate\u0026lt;String, String\u0026gt; predicateB = (key, value) -\u0026gt; value.startsWith(\u0026#34;い\u0026#34;); public static Topology getTopology() { StreamsBuilder builder = new StreamsBuilder(); @SuppressWarnings(\u0026#34;unchecked\u0026#34;) KStream\u0026lt;String, String\u0026gt;[] branchedStream = builder .stream(\u0026#34;input-topic\u0026#34;, Consumed.with(Serdes.String(), Serdes.String())) .branch(predicateA, predicateB); branchedStream[0].to(\u0026#34;output-topicA\u0026#34;); branchedStream[1].to(\u0026#34;output-topicB\u0026#34;); return builder.build(); }   branchを用いるとKStream型の配列を得ることができます。一番目のPredicate(predicateA)を通過したレコードは[0]、一番目は通過せず二番目(predicateB)を通過したPredicateが[1]に流れていきます。\n1 2 3 4 5 6 7 8 9 10  @Test void test() { inputRecord(\u0026#34;input-topic\u0026#34;, \u0026#34;key1\u0026#34;, \u0026#34;あひる\u0026#34;); inputRecord(\u0026#34;input-topic\u0026#34;, \u0026#34;key1\u0026#34;, \u0026#34;いんこ\u0026#34;); inputRecord(\u0026#34;input-topic\u0026#34;, \u0026#34;key1\u0026#34;, \u0026#34;からす\u0026#34;); System.out.println(getOutputRecord(\u0026#34;output-topicA\u0026#34;)); System.out.println(getOutputRecord(\u0026#34;output-topicA\u0026#34;)); System.out.println(getOutputRecord(\u0026#34;output-topicB\u0026#34;)); System.out.println(getOutputRecord(\u0026#34;output-topicB\u0026#34;)); }   1 2 3 4  topic=output-topicA, key=key1, value=あひる null topic=output-topicB, key=key1, value=いんこ null   \u0026ldquo;あひる\u0026quot;はoutput-topicA、\u0026ldquo;いんこ\u0026quot;はoutput-topicB、\u0026ldquo;からす\u0026quot;は除外される事が確認できました。 この記事の後半に登場するmergeと供に、非常に便利な機能です。\nMap レコードの内容を変換する関数です。 key,valueを引数にとるKeyValueMapper、valueのみを引数にとるValueMapperが用意されており、それぞれJava StreamのBiFunction、Functionとほぼ同じです。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27  private static KeyValueMapper\u0026lt;String, String, KeyValue\u0026lt;String, String\u0026gt;\u0026gt; keyValueMapper = (key, value) -\u0026gt; KeyValue.pair(key + \u0026#34;_hoge\u0026#34;, value + \u0026#34;_fuga\u0026#34;); private static ValueMapper\u0026lt;String, String\u0026gt; valueMapper = (value) -\u0026gt; value + \u0026#34;_foo\u0026#34;; private static KeyValueMapper\u0026lt;String, String, String\u0026gt; keyMapper = (key, value) -\u0026gt; key + \u0026#34;_\u0026#34; + value + \u0026#34;_bar\u0026#34;; public static Topology getTopology() { StreamsBuilder builder = new StreamsBuilder(); KStream\u0026lt;String, String\u0026gt; stream = builder .stream(\u0026#34;input-topic\u0026#34;, Consumed.with(Serdes.String(), Serdes.String())); stream .map(keyValueMapper) .to(\u0026#34;output-topicA\u0026#34;); stream .mapValues(valueMapper) .to(\u0026#34;output-topicB\u0026#34;); stream .selectKey(keyMapper) .to(\u0026#34;output-topicC\u0026#34;); return builder.build(); }   ここでは、keyとvalueを両方変換(output-topicA)、valueのみ変換(output-topicB)、keyのみ変換(output-topicC)の3パターンを試してみます。 それぞれ.map,.mapValues,.selectKeyで呼び出しますが、.selectKeyの引数はKeyValueMapperなのが少し紛らわしいところです。\nkeyとvalueを両方変換する場合、KeyValueMapperの帰り値はKeyValue型を使用します。入力と異なる型を返す場合は終端処理.toでシリアライザを指定する必要があります。\n(例:.to(\u0026quot;output-topic\u0026quot;,Produced.with(Serdes.Integer(),Serdes.Integer()))\nこれまでkeyに関して何も触れて来ませんでしたが、ステートフルな処理で多用するためその折に解説します。またKafkaクラスタを組んだ際、レコードがどのノードに送られるかを決定する値でもあります。\n1 2 3 4 5 6 7  @Test void test() { inputRecord(\u0026#34;input-topic\u0026#34;, \u0026#34;key1\u0026#34;, \u0026#34;value1\u0026#34;); System.out.println(getOutputRecord(\u0026#34;output-topicA\u0026#34;)); System.out.println(getOutputRecord(\u0026#34;output-topicB\u0026#34;)); System.out.println(getOutputRecord(\u0026#34;output-topicC\u0026#34;)); }   1 2 3  topic=output-topicA, key=key1_hoge, value=value1_fuga topic=output-topicB, key=key1, value=value1_foo topic=output-topicC, key=key1_value1_bar, value=value1   key\u0026amp;value、valueのみ、keyのみの変換がそれぞれ行われている事が確認できました。\nFlatMap Java Streamと同様にflatMapも用意されています。 1つのレコードをIterableな値に変換する事で、レコードごと複数に分割する機能です。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  private static ValueMapper\u0026lt;String, List\u0026lt;String\u0026gt;\u0026gt; valueMapper = (value) -\u0026gt; Arrays.asList(value.split(\u0026#34;:\u0026#34;)); public static Topology getTopology() { StreamsBuilder builder = new StreamsBuilder(); KStream\u0026lt;String, String\u0026gt; stream = builder .stream(\u0026#34;input-topic\u0026#34;, Consumed.with(Serdes.String(), Serdes.String())); stream .flatMapValues(valueMapper) .to(\u0026#34;output-topic\u0026#34;); return builder.build(); }   1 2 3 4 5 6 7  @Test void test() { inputRecord(\u0026#34;input-topic\u0026#34;, \u0026#34;key1\u0026#34;, \u0026#34;あ:い:う\u0026#34;); System.out.println(getOutputRecord(\u0026#34;output-topic\u0026#34;)); System.out.println(getOutputRecord(\u0026#34;output-topic\u0026#34;)); System.out.println(getOutputRecord(\u0026#34;output-topic\u0026#34;)); }   1 2 3  topic=output-topic, key=key1, value=あ topic=output-topic, key=key1, value=い topic=output-topic, key=key1, value=う   ForeachAction Java StreamのBiConsumerと同等で、戻り値のない処理を行います。 .peekでは、レコードをread onlyとして何らかの処理を行います。（主にロギングに使用されるかと思います) .forEachは.toと同様の終端処理として機能します。 終端処理が実施されるとKafkaにおいてそのレコードのConsumeが完了したと見なされます。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  private static ForeachAction\u0026lt;String, String\u0026gt; foreachAction = (key, value) -\u0026gt; System.out.println(\u0026#34;print (\u0026#34; + key + \u0026#34; : \u0026#34; + value + \u0026#34;)\u0026#34;); public static Topology getTopology() { StreamsBuilder builder = new StreamsBuilder(); KStream\u0026lt;String, String\u0026gt; stream = builder .stream(\u0026#34;input-topic\u0026#34;, Consumed.with(Serdes.String(), Serdes.String())); stream .peek(foreachAction) .map((key, value) -\u0026gt; KeyValue.pair(key + \u0026#34;_changed\u0026#34;, value + \u0026#34;_changed\u0026#34;)) .foreach(foreachAction); return builder.build(); }   1 2 3 4  @Test void test() { inputRecord(\u0026#34;input-topic\u0026#34;, \u0026#34;key1\u0026#34;, \u0026#34;ほげほげ\u0026#34;); }   1 2  print (key1 : ほげほげ) print (key1_changed : ほげほげ_changed)   レコードの内容がコンソールに出力されました。またforeachによってレコードのConsumeは完了し、Kafkaのオフセットが進行します。\nthrough レコードの内容をトピックに書き込み、さらに後続処理に渡します。 処理途中のデータから別のストリームを開始したい場合に使用します。\n1 2 3 4 5 6 7 8 9 10 11 12  public static Topology getTopology() { StreamsBuilder builder = new StreamsBuilder(); KStream\u0026lt;String, String\u0026gt; stream = builder .stream(\u0026#34;input-topic\u0026#34;, Consumed.with(Serdes.String(), Serdes.String())); stream .through(\u0026#34;output-topicA\u0026#34;) .map((key, value) -\u0026gt; KeyValue.pair(key + \u0026#34;_changed\u0026#34;, value + \u0026#34;_changed\u0026#34;)) .to(\u0026#34;output-topicB\u0026#34;); return builder.build(); }   1 2 3 4 5 6  @Test void test() { inputRecord(\u0026#34;input-topic\u0026#34;, \u0026#34;key1\u0026#34;, \u0026#34;ほげほげ\u0026#34;); System.out.println(getOutputRecord(\u0026#34;output-topicA\u0026#34;)); System.out.println(getOutputRecord(\u0026#34;output-topicB\u0026#34;)); }   1 2  topic=output-topicA, key=key1, value=ほげほげ topic=output-topicB, key=key1_changed, value=ほげほげ_changed   1つのレコードが形を変え2つのトピックに送られました。\nmerge 2つのストリームを1つに結合します。\nとりあえず実行してみましょう。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14  public static Topology getTopology() { StreamsBuilder builder = new StreamsBuilder(); KStream\u0026lt;String, String\u0026gt; streamA = builder .stream(\u0026#34;input-topicA\u0026#34;, Consumed.with(Serdes.String(), Serdes.String())); KStream\u0026lt;String, String\u0026gt; streamB = builder .stream(\u0026#34;input-topicB\u0026#34;, Consumed.with(Serdes.String(), Serdes.String())); streamA .merge(streamB) .to(\u0026#34;output-topic\u0026#34;); return builder.build(); }   1 2 3 4 5 6 7  @Test void test() { inputRecord(\u0026#34;input-topicA\u0026#34;, \u0026#34;key1\u0026#34;, \u0026#34;ほげほげ\u0026#34;); inputRecord(\u0026#34;input-topicB\u0026#34;, \u0026#34;key1\u0026#34;, \u0026#34;ふがふが\u0026#34;); System.out.println(getOutputRecord(\u0026#34;output-topic\u0026#34;)); System.out.println(getOutputRecord(\u0026#34;output-topic\u0026#34;)); }   1 2  topic=output-topic, key=key1, value=ほげほげ topic=output-topic, key=key1, value=ふがふが   異なるトピックに入れたレコードが一つのストリームになりました。\nKafka Streams FizzBuzzを実装する 最後に、これまでの知識を無駄に使って冗長なFizzBuzzを実装してみましょう。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31  @SuppressWarnings(\u0026#34;unchecked\u0026#34;) public static Topology getTopology() { StreamsBuilder builder = new StreamsBuilder(); KStream\u0026lt;Integer, String\u0026gt; inputStream = builder .stream(\u0026#34;input-topic\u0026#34;, Consumed.with(Serdes.String(), Serdes.Integer())) .map((key, value) -\u0026gt; KeyValue.pair(value, \u0026#34;\u0026#34;)); KStream\u0026lt;Integer, String\u0026gt; branchFizz[] = inputStream .branch((key, value) -\u0026gt; key % 3 == 0, (key, value) -\u0026gt; true); KStream\u0026lt;Integer, String\u0026gt; branchBuzz[] = branchFizz[0] .mapValues(value-\u0026gt;\u0026#34;Fizz\u0026#34;) .merge(branchFizz[1]) .branch((key, value) -\u0026gt; key % 5 == 0, (key, value) -\u0026gt; true); KStream\u0026lt;Integer, String\u0026gt; branchFizzBuzz[] = branchBuzz[0] .mapValues(value -\u0026gt; value + \u0026#34;Buzz\u0026#34;) .merge(branchBuzz[1]) .branch((key,value)-\u0026gt;value.equals(\u0026#34;\u0026#34;), (key,value)-\u0026gt;true); branchFizzBuzz[0] .mapValues((key, value) -\u0026gt; String.valueOf(key)) .merge(branchFizzBuzz[1]) .to(\u0026#34;output-topic\u0026#34;, Produced.with(Serdes.Integer(), Serdes.String())); return builder.build(); }   1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18  private void inputRecord(String topicName, String key, Integer value) { testDriver.pipeInput( new ConsumerRecordFactory\u0026lt;String, Integer\u0026gt;(Serdes.String().serializer(), Serdes.Integer().serializer()).create(topicName, key, value)); } private ProducerRecord\u0026lt;Integer, String\u0026gt; getOutputRecord(String topicName) { return testDriver.readOutput(topicName, Serdes.Integer().deserializer(), Serdes.String().deserializer()); } @Test void test() { for (int i = 1; i \u0026lt;= 30; i++) { inputRecord(\u0026#34;input-topic\u0026#34;, \u0026#34;key1\u0026#34;, i); System.out.println(getOutputRecord(\u0026#34;output-topic\u0026#34;).value()); } }   いかがでしたでしょうか。\n次回はいよいよステートフルな処理を試してみようと思います。\n","permalink":"https://aaaanwz.github.io/post/2019/kafkastreams-2/","summary":"Kafka Streams DSLのうち、ステートレスな操作(branch,map,mergeなど)を実際に触り、動作を確認します。 また最後に、本稿で登場する関数を使用してストリーム処理のFizzBuzzを実装してみます。\n前回の記事(準備編)のプロジェクトが作成済みである事を前提とします。\n実際にやってみる Filter Java StreamのByPredicateと同じと思って差し支えありません。Java StreamのPredicateと紛らわしいのでimport対象に注意しましょう。\nkey, valueを引数にbooleanを返し、falseの場合はレコードが除外されます。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15  import org.apache.kafka.streams.kstream.Predicate; ... private static Predicate\u0026lt;String, String\u0026gt; predicate = (key, value) -\u0026gt; value.startsWith(\u0026#34;あ\u0026#34;); public static Topology getTopology() { StreamsBuilder builder = new StreamsBuilder(); builder .stream(\u0026#34;input-topic\u0026#34;, Consumed.with(Serdes.String(), Serdes.String())) //使用するデシリアライザにStringを明示指定します .filter(predicate) .to(\u0026#34;output-topic\u0026#34;); return builder.build(); }   ここでConsumed.with(...)が新たに登場しました。Predicateの引数がString型なので、デシリアライザも明示指定する必要があるためです。\nまた.filter((key, value)-\u0026gt; value.startsWith(\u0026quot;あ\u0026quot;))のように直接ラムダ式を記述することももちろん可能です。\nさて、テストを実行してみましょう。\n1 2 3 4 5 6 7  @Test void test() { inputRecord(\u0026#34;input-topic\u0026#34;, \u0026#34;key1\u0026#34;, \u0026#34;あけまして\u0026#34;); System.","title":"Kafka Streams DSLを一通り体験する (2. ステートレス処理実践編)"},{"content":"Kafka Streamsを使ってステートフルなストリーム処理を実装したいと思い立ったものの、Kafka Streams Developer guideを読んでもいまいちよくわからなかったため、自分で一通り試してみました。\nこの記事ではAggregate Reduce Join Windowingなど、Kafka Streams DSLでできる事を順番にテストし、挙動を確認していきます。また、kafka-streams-test-utilsを用いたJUnitの実装についても解説します。\n開発環境  OpenJDK 11 Maven 3.6 Kafka 2.1.1  下準備 プロジェクトの作成 以下の依存関係を追加します\n kafka-streams kafka-streams-test-utils junit-jupiter-api junit-jupiter-engine maven-surefire-plugin  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48  \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34; xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt; \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt; \u0026lt;groupId\u0026gt;aaaanwz\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;kstream-dsl-test\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt; \u0026lt;properties\u0026gt; \u0026lt;maven.compiler.source\u0026gt;11\u0026lt;/maven.compiler.source\u0026gt; \u0026lt;maven.compiler.target\u0026gt;11\u0026lt;/maven.compiler.target\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencies\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.kafka\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;kafka-streams\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.2.1\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.apache.kafka\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;kafka-streams-test-utils\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;2.2.1\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.junit.jupiter\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit-jupiter-api\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.4.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;dependency\u0026gt; \u0026lt;groupId\u0026gt;org.junit.jupiter\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;junit-jupiter-engine\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;5.4.0\u0026lt;/version\u0026gt; \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt; \u0026lt;/dependency\u0026gt; \u0026lt;/dependencies\u0026gt; \u0026lt;build\u0026gt; \u0026lt;pluginManagement\u0026gt; \u0026lt;plugins\u0026gt; \u0026lt;plugin\u0026gt; \u0026lt;groupId\u0026gt;org.apache.maven.plugins\u0026lt;/groupId\u0026gt; \u0026lt;artifactId\u0026gt;maven-surefire-plugin\u0026lt;/artifactId\u0026gt; \u0026lt;version\u0026gt;3.0.0-M3\u0026lt;/version\u0026gt; \u0026lt;/plugin\u0026gt; \u0026lt;/plugins\u0026gt; \u0026lt;/pluginManagement\u0026gt; \u0026lt;/build\u0026gt; \u0026lt;/project\u0026gt;   Mainクラスの作成 まずはじめに、input-topicに入ってきたレコードをそのままoutput-topicに送るストリーム処理(Topology)を実装します。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  package core; import org.apache.kafka.streams.StreamsBuilder; import org.apache.kafka.streams.Topology; public class Main { public static void main(String[] args) { } public static Topology getTopology() { StreamsBuilder builder = new StreamsBuilder(); builder .stream(\u0026#34;input-topic\u0026#34;) //\u0026#39;input-topic\u0026#39;からレコードを取得します  .to(\u0026#34;output-topic\u0026#34;); //\u0026#39;output-topic\u0026#39;にレコードを書き込みます  return builder.build(); } }   JUnitテストの作成 Main#getTopology()の挙動を確認するJUnitを実装します。 kafka-streams-test-utilsを用いると、実際のKafka環境がなくてもテストが可能です。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64  package core; import java.util.Properties; import org.apache.kafka.clients.producer.ProducerRecord; import org.apache.kafka.common.serialization.Serdes; import org.apache.kafka.streams.StreamsConfig; import org.apache.kafka.streams.TopologyTestDriver; import org.apache.kafka.streams.test.ConsumerRecordFactory; import org.junit.jupiter.api.Test; class MainTest { /** * Kafkaへの接続情報。 */ private static final Properties properties; static { properties = new Properties(); properties.put(StreamsConfig.APPLICATION_ID_CONFIG, \u0026#34;kafka-dsl-test\u0026#34;); properties.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \u0026#34;dummy:1234\u0026#34;);//実際のKafkaには接続しないため、適当な値でOK  properties.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName()); properties.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName()); } /** * TestDriverの準備 */ private static TopologyTestDriver testDriver; @BeforeEach void before() { testDriver = new TopologyTestDriver(Main.getTopology(), properties); } @AfterEach void after() { testDriver.close(); } /** * 入力レコードをシリアライズするFactoryを使用し、TopologyTestDriverのinput-topicに入力します。 * ConsumerRecordFactoryインスタンスは都度生成しないと内部のタイムスタンプが更新されず、後々のステートフルな処理がうまく動作しないため注意が必要です。 */ private void inputRecord(String topicName, String key,String value) { testDriver.pipeInput( new ConsumerRecordFactory\u0026lt;String,String\u0026gt;(Serdes.String().serializer(),Serdes.String().serializer()).create(topicName,key,value) ); } /* * TopologyTestDriverのoutput-topicのレコードを取得し、Stringにデシリアライズします */ private ProducerRecord\u0026lt;String,String\u0026gt; getOutputRecord(String topicName){ return testDriver.readOutput(topicName,Serdes.String().deserializer(),Serdes.String().deserializer()); } @Test void test() { inputRecord(\u0026#34;input-topic\u0026#34;, \u0026#34;key1\u0026#34;, \u0026#34;ほげほげ\u0026#34;); ProducerRecord\u0026lt;String, String\u0026gt; output = getOutputRecord(\u0026#34;output-topic\u0026#34;); System.out.println(output);//\u0026#39;output-topic\u0026#39;の内容をコンソールに表示します。  } }   mvn testで実行します。以下がコンソールに出力されましたでしょうか？\nProducerRecord(topic=output-topic, partition=null, headers=RecordHeaders(headers = [], isReadOnly = false), key=key1, value=ほげほげ, timestamp=1559637393116) レコードのtopic, key, value, timestampがそれぞれ確認できますね。\n問題なさそうであれば、本物のKafkaで動作させてみましょう。 Main#main()に追記します。 BOOTSTRAP_SERVER_CONFIGの値localhost:9092は各自の環境に合わせて置き換えてください。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56  package core; import java.util.Properties; import java.util.concurrent.CountDownLatch; import org.apache.kafka.common.serialization.Serdes; import org.apache.kafka.streams.KafkaStreams; import org.apache.kafka.streams.StreamsBuilder; import org.apache.kafka.streams.StreamsConfig; import org.apache.kafka.streams.Topology; public class Main { /** * Kafkaへの接続情報。 */ private static final Properties properties; static { properties = new Properties(); properties.put(StreamsConfig.APPLICATION_ID_CONFIG, \u0026#34;kafka-dsl\u0026#34;); properties.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, \u0026#34;localhost:9092\u0026#34;); properties.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName()); properties.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName()); } public static void main(String[] args) throws InterruptedException { KafkaStreams streams = new KafkaStreams(getTopology(),properties); streams.setUncaughtExceptionHandler((Thread t, Throwable e) -\u0026gt; { e.printStackTrace(); System.exit(1); }); final CountDownLatch latch = new CountDownLatch(1); Runtime.getRuntime().addShutdownHook(new Thread(\u0026#34;streams-shutdown-hook\u0026#34;) { @Override public void run() { streams.close(); latch.countDown(); } }); streams.start(); latch.await(); } public static Topology getTopology() { StreamsBuilder builder = new StreamsBuilder(); builder .stream(\u0026#34;input-topic\u0026#34;) //topic名\u0026#39;input-topic\u0026#39;からレコードを取得します  .to(\u0026#34;output-topic\u0026#34;); //topic名 \u0026#39;output-topic\u0026#39;にレコードを書き込みます  return builder.build(); } }   実行できましたら、kafka-console-producer とkafka-console-consumerで動作を確認してみてください。(参考:Kafka Quickstart) input-topicに送ったレコードと同じ内容をoutput-topicから受け取れるかと思います。\n次回はmain#getTopology()に様々な関数を追加し、挙動をチェックしていきます。\n","permalink":"https://aaaanwz.github.io/post/2019/kafkastreams-1/","summary":"Kafka Streamsを使ってステートフルなストリーム処理を実装したいと思い立ったものの、Kafka Streams Developer guideを読んでもいまいちよくわからなかったため、自分で一通り試してみました。\nこの記事ではAggregate Reduce Join Windowingなど、Kafka Streams DSLでできる事を順番にテストし、挙動を確認していきます。また、kafka-streams-test-utilsを用いたJUnitの実装についても解説します。\n開発環境  OpenJDK 11 Maven 3.6 Kafka 2.1.1  下準備 プロジェクトの作成 以下の依存関係を追加します\n kafka-streams kafka-streams-test-utils junit-jupiter-api junit-jupiter-engine maven-surefire-plugin  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48  \u0026lt;project xmlns=\u0026#34;http://maven.","title":"Kafka Streams DSLを一通り体験する(1. 準備編)"},{"content":"resumeはDM等でお送りします.\n2022/01 ~ データ分析  BigQuery ML Vertex AI Workbench   2021/09 ~ 2021/12 Webサービス データ基盤新規開発 Data pipeline  Kinesis Firehose Argo Workflows embulk Fluentd AWS Database Migration Service  DB/DWH/Datalake  S3 MySQL BigQuery  Infra  terraform EKS Fargate  Argo CD    BI  Redash   2020/01 ~ 2021/08 エンタープライズ データ基盤刷新 Data pipeline  Kinesis Lambda Pub/Sub Dataflow  Apache Beam   Apache Airflow  DB/DWH/Datalake  S3 GCS MySQL RedShift BigQuery Elasticsearch  BI  Readash Kibana Jupyterhub  Infra  CircleCI Prometheus Grafana  Language  Node.js golang Java   2018/10 ~ 2020/11 DMPサービス インフラ/バックエンド刷新 Data pipeline  Apache Kafka  Kafka Streams    DB/DWH/Datalake  S3 Apache Cassandra MySQL Athena  Infra  EKS  Argo CD   CircleCI Datadog  Language  Java   ~ 2018/09 レガシー系ミドルウェア製品開発/保守  IBM z/OS DB2 for z/OS アセンブリ言語 (z/Architecture) Dump analysis (ISPF)   ","permalink":"https://aaaanwz.github.io/portfolio/","summary":"resumeはDM等でお送りします.\n2022/01 ~ データ分析  BigQuery ML Vertex AI Workbench   2021/09 ~ 2021/12 Webサービス データ基盤新規開発 Data pipeline  Kinesis Firehose Argo Workflows embulk Fluentd AWS Database Migration Service  DB/DWH/Datalake  S3 MySQL BigQuery  Infra  terraform EKS Fargate  Argo CD    BI  Redash   2020/01 ~ 2021/08 エンタープライズ データ基盤刷新 Data pipeline  Kinesis Lambda Pub/Sub Dataflow  Apache Beam   Apache Airflow  DB/DWH/Datalake  S3 GCS MySQL RedShift BigQuery Elasticsearch  BI  Readash Kibana Jupyterhub  Infra  CircleCI Prometheus Grafana  Language  Node.","title":"Portfolio"}]