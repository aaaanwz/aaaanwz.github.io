<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/">
  <channel>
    <title>ETL on A4 tech note</title>
    <link>https://aaaanwz.github.io/categories/etl/</link>
    <description>Recent content in ETL on A4 tech note</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja</language>
    <lastBuildDate>Thu, 28 Oct 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://aaaanwz.github.io/categories/etl/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>embulkをArgo workflowsで実行するTemplate</title>
      <link>https://aaaanwz.github.io/post/2021/argo-workflows-embulk/</link>
      <pubDate>Thu, 28 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://aaaanwz.github.io/post/2021/argo-workflows-embulk/</guid>
      <description>Argo Workflowsの公式ドキュメントが分かりづらかったので、試しにembulkを実行するテンプレートを作ってみました。
config.ymlはartifactsとして渡します。
Dockerfile 1 2 3 4 5 6 7 8 9 10 11 12 13  FROM openjdk:8-jre-alpine ARG VERSION=latest RUN mkdir -p /root/.embulk/bin \ &amp;amp;&amp;amp; wget -q https://dl.embulk.org/embulk-${VERSION}.jar -O /root/.embulk/bin/embulk \ &amp;amp;&amp;amp; chmod +x /root/.embulk/bin/embulk ENV PATH=$PATH:/root/.embulk/bin RUN apk add --no-cache libc6-compat RUN embulk gem install embulk-input-s3 ENTRYPOINT [&amp;#34;java&amp;#34;, &amp;#34;-jar&amp;#34;, &amp;#34;/root/.embulk/bin/embulk&amp;#34;]   1 2 3  $ EMBULK_VERSION=0.9.23 $ docker build . -t embulk:$EMBULK_VERSION --build-arg VERSION=$EMBULK_VERSION $ docker run -v /path/to/configfile:/config embulk:latest run /config/config.</description>
    </item>
    
    <item>
      <title>[fluetd] S3にアップロードされたキー名でルーティングする</title>
      <link>https://aaaanwz.github.io/post/2021/fluentd-s3-routing/</link>
      <pubDate>Tue, 26 Oct 2021 00:00:00 +0000</pubDate>
      
      <guid>https://aaaanwz.github.io/post/2021/fluentd-s3-routing/</guid>
      <description>S3にアップロードされたファイルをfluentdでBigQueryにinsertする際、S3キー名に応じてテーブルを振り分けるサンプルを掲載します。 ここではフォーマットはs3://my-bucket/{BigQueryデータセット名}/{テーブル名}/{uuid}.csv.gz とします。
fluent.conf 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46  &amp;lt;source&amp;gt; tag s3 @type s3 s3_bucket my-bucket s3_region ap-northeast-1 &amp;lt;sqs&amp;gt; queue_name my-queue &amp;lt;/sqs&amp;gt; &amp;lt;/source&amp;gt; &amp;lt;match s3&amp;gt; @type rewrite_tag_filter &amp;lt;rule&amp;gt; key s3_key pattern ^(.</description>
    </item>
    
  </channel>
</rss>
